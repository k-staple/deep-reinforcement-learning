{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport agent, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.21 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from workspace_utils import active_session\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726671e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,  -4.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,  -0.00000000e+00,  -0.00000000e+00,\n",
       "         -4.37113883e-08,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,  -1.00000000e+01,\n",
       "          0.00000000e+00,   1.00000000e+00,  -0.00000000e+00,\n",
       "         -0.00000000e+00,  -4.37113883e-08,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   5.75471878e+00,\n",
       "         -1.00000000e+00,   5.55726671e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,  -1.68164849e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \\nstates = env_info.vector_observations                  # get the current state (for each agent)\\nscores = np.zeros(num_agents)                          # initialize the score (for each agent)\\niteration = 0\\nwhile True:\\n    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\\n    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\\n    print(\"\\t\", actions)\\n    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\\n    next_states = env_info.vector_observations         # get next state (for each agent)\\n    rewards = env_info.rewards                         # get reward (for each agent)\\n    if rewards[0] != 0:\\n        print(\"rewards\", rewards)\\n    dones = env_info.local_done                        # see if episode finished\\n    scores += env_info.rewards                         # update the score (for each agent)\\n    states = next_states                               # roll over states to next time step\\n    if np.any(dones):                                  # exit loop if episode finished\\n        print(iteration)\\n        break\\n    iteration += 1\\nprint(\\'Total score (averaged over agents) this episode: {}\\'.format(np.mean(scores)))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "iteration = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    print(\"\\t\", actions)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    if rewards[0] != 0:\n",
    "        print(\"rewards\", rewards)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        print(iteration)\n",
    "        break\n",
    "    iteration += 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nimport queue\\nfrom collections import deque\\n\\nprint_every = 200\\n# seems to be 1000 for the env anyway\\nmax_ts = 2000\\nmax_episodes = 1000\\ncurr_agent = agent.Agent(state_size, action_size)\\n\\nenv_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \\nstate = env_info.vector_observations[0]                  # get the current state (for each agent)\\n# can\\'t use a queue.Queue because \"Insertion will block once this size has been reached, until queue items are consumed\"\\nscores = deque(maxlen=100)                          # initialize the score (for each agent)\\nscores_history = []\\n\\nepisode_won_i = 0\\n\\nwith active_session():\\n    # TODO: maybe set max # of episodes\\n    for i in range(max_episodes):\\n        score = 0\\n        for t in range(max_ts):\\n            action = curr_agent.act(state.astype(\\'float32\\', copy=False))\\n            \\n            # because random numpy actions at the beginning are already numpy\\n            \\n            try:\\n                action = action.to(\"cpu\").detach().numpy()\\n            except:\\n                pass\\n            if t % print_every == 0:\\n                print(\\'\\taction\\', action)\\n            env_info = env.step(action)[brain_name]\\n            reward = env_info.rewards[0]\\n            if reward != 0:\\n                print(\"reward\", reward)\\n            next_state = env_info.vector_observations[0]\\n            done = env_info.local_done[0]\\n            \\n            score = score * curr_agent.Q_DISCOUNT + reward\\n\\n            curr_agent.step(state, action, reward, next_state, done)\\n\\n\\n            if done: \\n                print(\"episode {} at {} ts; done reached\".format(i, t))\\n                break\\n        scores_history.append(score)\\n        scores.append(score)\\n        if i % print_every == 0:\\n            print(\"episode {}; average score past 100 episodes: {}\".format(i, np.mean(scores)))\\n        if np.mean(scores) >= 30:\\n            episode_won_i = i\\n            print(\"Solved in {} episodes\".format(episode_won_i))\\n            curr_agent.save()\\n            break\\n        \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "import queue\n",
    "from collections import deque\n",
    "\n",
    "print_every = 200\n",
    "# seems to be 1000 for the env anyway\n",
    "max_ts = 2000\n",
    "max_episodes = 1000\n",
    "curr_agent = agent.Agent(state_size, action_size)\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "# can't use a queue.Queue because \"Insertion will block once this size has been reached, until queue items are consumed\"\n",
    "scores = deque(maxlen=100)                          # initialize the score (for each agent)\n",
    "scores_history = []\n",
    "\n",
    "episode_won_i = 0\n",
    "\n",
    "with active_session():\n",
    "    # TODO: maybe set max # of episodes\n",
    "    for i in range(max_episodes):\n",
    "        score = 0\n",
    "        for t in range(max_ts):\n",
    "            action = curr_agent.act(state.astype('float32', copy=False))\n",
    "            \n",
    "            # because random numpy actions at the beginning are already numpy\n",
    "            \n",
    "            try:\n",
    "                action = action.to(\"cpu\").detach().numpy()\n",
    "            except:\n",
    "                pass\n",
    "            if t % print_every == 0:\n",
    "                print('\\taction', action)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            reward = env_info.rewards[0]\n",
    "            if reward != 0:\n",
    "                print(\"reward\", reward)\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            score = score * curr_agent.Q_DISCOUNT + reward\n",
    "\n",
    "            curr_agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "            if done: \n",
    "                print(\"episode {} at {} ts; done reached\".format(i, t))\n",
    "                break\n",
    "        scores_history.append(score)\n",
    "        scores.append(score)\n",
    "        if i % print_every == 0:\n",
    "            print(\"episode {}; average score past 100 episodes: {}\".format(i, np.mean(scores)))\n",
    "        if np.mean(scores) >= 30:\n",
    "            episode_won_i = i\n",
    "            print(\"Solved in {} episodes\".format(episode_won_i))\n",
    "            curr_agent.save()\n",
    "            break\n",
    "        \n",
    "'''     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taction [ 0.07024846  0.06046766  0.05394188  0.07938849]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/agent.py:108: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.local_critic.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taction [ 0.56718749  0.44627762  0.50413519  0.55207491]\n",
      "\taction [ 0.31182331  0.18612283  0.51040828  0.52188283]\n",
      "\taction [ 0.1014202  -0.01392245 -0.01156328  0.13201006]\n",
      "episode 0 at 1000 ts; done reached\n",
      "episode 0; average score past 100 episodes: 0.0\n",
      "\taction [ 0.03889588 -0.2812323  -0.08822398 -0.07975279]\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.10934111 -0.21620239 -0.0782683  -0.04887635]\n",
      "\taction [-0.30097932 -0.28838116  0.16330023  0.18764535]\n",
      "\taction [-0.3132247  -0.29150179  0.55833799  0.29511431]\n",
      "episode 1 at 1000 ts; done reached\n",
      "\taction [-0.26680923 -0.50978571  0.11294265  0.13137348]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "\taction [ 0.03480282 -0.27685443  0.51353616  0.72235864]\n",
      "\taction [ 0.12527095 -0.0548347   0.20689675  1.        ]\n",
      "\taction [ 0.19304891  0.03299007  0.43960345  0.15112726]\n",
      "episode 2 at 1000 ts; done reached\n",
      "\taction [-0.14316145 -0.14693931 -0.35964254 -0.17915908]\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "\taction [ 0.55856073  0.64387453  0.16627891  0.46161407]\n",
      "\taction [ 1.          0.95198464 -0.09814474 -0.22589587]\n",
      "\taction [ 1.          1.         -0.20597982 -0.12867479]\n",
      "episode 3 at 1000 ts; done reached\n",
      "\taction [ 0.68159026  0.46838546 -0.5875091  -0.65626067]\n",
      "\taction [ 1.          1.         -0.4530378  -0.29999244]\n",
      "\taction [ 1.          0.68856961  0.73465574 -0.3848483 ]\n",
      "\taction [ 1.          0.92973244  1.         -0.35520303]\n",
      "episode 4 at 1000 ts; done reached\n",
      "\taction [ 0.27363628 -0.10293293  0.42674759 -0.6812101 ]\n",
      "\taction [ 0.05823884 -0.28433618  1.         -0.25259483]\n",
      "\taction [-0.35679305 -0.12027547  1.         -0.25042856]\n",
      "\taction [-0.24101564 -0.55016363  1.         -0.60463285]\n",
      "episode 5 at 1000 ts; done reached\n",
      "\taction [-0.48796597 -0.59217584  0.83707225 -0.8877359 ]\n",
      "\taction [-0.06012617 -0.56182921  1.         -0.32767576]\n",
      "\taction [-0.37472495 -0.23305321  1.         -0.25310642]\n",
      "\taction [-0.33250183 -0.28634781  1.         -0.27239755]\n",
      "episode 6 at 1000 ts; done reached\n",
      "\taction [-0.34661978 -0.74503607  0.86719018 -0.81236744]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.3295033  -0.32138395  1.         -0.25782159]\n",
      "\taction [-0.21354333 -0.42977211  1.         -0.35146034]\n",
      "\taction [-0.14393124 -0.36992136  1.         -0.44912013]\n",
      "episode 7 at 1000 ts; done reached\n",
      "\taction [-0.23773628 -0.77360588  0.96536469 -0.70560777]\n",
      "\taction [ 0.45937136 -0.41337287  1.         -0.48556682]\n",
      "\taction [-0.22216733 -0.40481561  1.         -0.45805961]\n",
      "\taction [-0.25000632 -0.53345448  1.         -0.45971557]\n",
      "episode 8 at 1000 ts; done reached\n",
      "\taction [-0.17869544 -0.58806372  0.80720025 -0.80494654]\n",
      "\taction [-0.4039453  -0.0944461   1.         -0.24606946]\n",
      "\taction [ 1.         -0.34610468  1.         -0.2203546 ]\n",
      "\taction [-0.12850377 -0.35953194  1.         -0.3694762 ]\n",
      "episode 9 at 1000 ts; done reached\n",
      "\taction [-0.27057686 -0.63721079  0.83352351 -0.62506026]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 0.93708754 -0.35647526  1.         -0.10682794]\n",
      "\taction [ 1.         -0.35464913  1.          0.06848458]\n",
      "\taction [-0.55949527 -0.38221264  1.         -0.14297372]\n",
      "episode 10 at 1000 ts; done reached\n",
      "\taction [-0.0207168  -0.56746179  0.68451107 -0.40797922]\n",
      "\taction [-0.38976997 -0.39377224  1.         -0.25132883]\n",
      "\taction [ 1.         -0.14403966  1.          1.        ]\n",
      "\taction [-0.42186317 -0.33999053  1.         -0.51707387]\n",
      "episode 11 at 1000 ts; done reached\n",
      "\taction [ 0.03845931 -0.64566028  0.49699146 -0.67876035]\n",
      "\taction [-0.15506369 -0.2130222   1.         -0.26917621]\n",
      "\taction [-0.20092621 -0.25539893  1.         -0.25787216]\n",
      "\taction [ 1.         -0.45194736  1.         -0.40177974]\n",
      "episode 12 at 1000 ts; done reached\n",
      "\taction [-0.07882645 -0.58987385  0.19216797 -0.4334662 ]\n",
      "\taction [-0.43436119 -0.33569333  1.         -0.24691373]\n",
      "\taction [-0.29060751 -0.47912514  1.         -0.09760948]\n",
      "\taction [-0.39742595 -0.29311776  1.         -0.37856719]\n",
      "episode 13 at 1000 ts; done reached\n",
      "\taction [ 0.02985251 -0.64129382 -0.11774907 -0.80621934]\n",
      "\taction [ 1.         -0.33510873  1.         -0.5826838 ]\n",
      "\taction [-0.58657408 -0.2473294   1.         -0.30875093]\n",
      "\taction [-0.3171792  -0.34881762  0.9672069  -0.21935572]\n",
      "episode 14 at 1000 ts; done reached\n",
      "\taction [-0.11549444 -0.63124359 -0.33279222 -0.65063047]\n",
      "\taction [ 1.         -0.16932048 -0.08844356 -0.18934794]\n",
      "\taction [-0.33711463 -0.47494662  1.         -0.29240865]\n",
      "\taction [ 1.         -0.3548345   0.90389347  0.15116373]\n",
      "episode 15 at 1000 ts; done reached\n",
      "\taction [-0.57595581 -0.61405772 -0.22940384 -0.44530925]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.33468768  0.7924875   0.47003549]\n",
      "\taction [-0.29055691 -0.31504974  1.         -0.31348008]\n",
      "\taction [ 1.         -0.1702181   0.17363313  0.22127929]\n",
      "episode 16 at 1000 ts; done reached\n",
      "\taction [ 0.09362536 -0.71065515 -0.74632561 -0.74735916]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.1578833   0.0991052   0.29710433]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.33521843 -0.38702098  0.33211958 -0.0536856 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.1287728  -0.17712851  0.19420305  0.88751435]\n",
      "episode 17 at 1000 ts; done reached\n",
      "\taction [ 0.35272014 -0.57078201 -0.74141115 -0.19467175]\n",
      "\taction [ 0.15765212 -0.27554074  0.57837474  0.63537824]\n",
      "\taction [-0.07298625 -0.25364599  0.98023349  1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.23424892 -0.27632469  1.          0.25490922]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "episode 18 at 1000 ts; done reached\n",
      "\taction [-0.49392068 -0.09598178 -0.66949767  0.42509693]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 0.11902062  0.0558528  -0.40956295  0.54387486]\n",
      "\taction [ 0.12752999 -0.04171355 -0.28572252  0.17849988]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.66668206  0.19515866 -0.0070925   0.74855328]\n",
      "episode 19 at 1000 ts; done reached\n",
      "\taction [-0.45134386 -0.04137135 -0.91140819  0.61231744]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.45897239 -0.24977681 -0.00606275  0.92385077]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.0574053  -0.34376281 -0.32121393  0.14292705]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.0288889  -0.29417649 -0.13521805]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "episode 20 at 1000 ts; done reached\n",
      "\taction [ 0.83294356 -0.73992693 -0.92529851 -0.75380474]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.12126962 -0.56393099 -0.52861881]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.51263529 -0.14609605 -0.21487954 -0.14898218]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.05619076  0.61616039 -0.42201778  0.81872547]\n",
      "episode 21 at 1000 ts; done reached\n",
      "\taction [ 0.46437508 -0.59588379 -0.96610337 -0.2409478 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.14466923 -0.49385938 -0.14309683]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.11526646 -0.35969663  0.01787884]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 0.87168092 -0.41952586 -0.2298501  -0.47799963]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "episode 22 at 1000 ts; done reached\n",
      "\taction [ 0.58075231  0.26416057 -0.95019931 -0.29367352]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "\taction [ 0.98063415 -0.26867244 -0.41639724 -0.13363555]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.08941047  1.         -0.24735329 -0.02464679]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.18301438 -0.08543538  0.39351982 -0.08732623]\n",
      "episode 23 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97456223 -0.99052858 -0.95891696]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.26290712 -0.48742005 -0.2187154 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.16319011  1.          0.19164351  0.22186342]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 0.35403591  0.07042643 -0.35741273 -0.12503906]\n",
      "episode 24 at 1000 ts; done reached\n",
      "\taction [ 0.96377385 -0.85774672 -0.92470896 -0.78844881]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.34331748 -0.37676844 -0.40462843]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 0.78007674  0.97847742 -0.1652178  -0.10653392]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.26781133  0.37153152  0.12834728 -0.11258852]\n",
      "episode 25 at 1000 ts; done reached\n",
      "\taction [-0.51967102 -0.1919266  -0.8333618  -0.32099196]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.31094605  0.19733094 -0.24562278  0.40069923]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.20672014 -0.0095269  -0.33175009 -0.20630926]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taction [-0.0664151  -0.30481663 -0.44472134 -0.45688978]\n",
      "episode 26 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.80614245 -0.96336704 -0.88440084]\n",
      "\taction [ 1.         -0.37064657 -0.55220127 -0.30660456]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.29925677 -0.01981183 -0.33007416 -0.09176   ]\n",
      "\taction [-0.30933478 -0.31353444 -0.3093321  -0.12278116]\n",
      "episode 27 at 1000 ts; done reached\n",
      "\taction [-0.71334076 -0.91612053 -0.92997646 -0.91778696]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.3715919  -0.5011664  -0.56091905 -0.30050665]\n",
      "\taction [-0.3953371  -0.14672913 -0.18565318 -0.33217138]\n",
      "\taction [-0.27146617 -0.35837162 -0.30828601 -0.24396622]\n",
      "episode 28 at 1000 ts; done reached\n",
      "\taction [-0.85420626 -0.82003081 -0.97280711 -0.76132941]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.20783924 -0.29423279 -0.37725767]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.02339284 -0.17712151 -0.46408087 -0.26643255]\n",
      "\taction [-0.59842038 -0.21837915 -0.20162509 -0.33519995]\n",
      "episode 29 at 1000 ts; done reached\n",
      "\taction [-0.66159678 -0.63726741 -0.88970268 -0.86645156]\n",
      "\taction [-0.29505885 -0.18227005 -0.31870934 -0.35861123]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.05319148 -0.21841365 -0.2385176  -0.15261011]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.54597354 -0.19773719 -0.23350421 -0.28314757]\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "episode 30 at 1000 ts; done reached\n",
      "\taction [-0.77327013 -0.75044715 -0.90418035 -0.84533918]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.5126074  -0.51082242 -0.27410072 -0.28319332]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 0.55424958 -0.1166578  -0.14008668 -0.13999413]\n",
      "\taction [-0.53967118 -0.22154957 -0.42134032 -0.37221524]\n",
      "episode 31 at 1000 ts; done reached\n",
      "\taction [-0.52249116 -0.8634634  -0.80071604 -0.97743452]\n",
      "\taction [-0.11920638 -0.34319043 -0.25237921 -0.32224989]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.47654173 -0.3303228  -0.56284565]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.2617909  -0.34331447 -0.44617885 -0.3587079 ]\n",
      "reward 0.019999999552965164\n",
      "episode 32 at 1000 ts; done reached\n",
      "\taction [-0.7829482  -0.89818406 -0.9113909  -0.88721448]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.06916495 -0.21851052 -0.42772636  0.07176547]\n",
      "\taction [-0.28655243 -0.32865602 -0.21147786 -0.43503317]\n",
      "\taction [-0.15615411 -0.25945547 -0.44084892 -0.32538047]\n",
      "episode 33 at 1000 ts; done reached\n",
      "\taction [-0.92986089 -0.95148891 -0.94273889 -0.7949158 ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.41506079 -0.51827967 -0.34481615 -0.33495018]\n",
      "\taction [-0.46356916 -0.35359839 -0.35196456 -0.31812492]\n",
      "\taction [-0.57690257 -0.34098512 -0.14019385 -0.37867135]\n",
      "episode 34 at 1000 ts; done reached\n",
      "\taction [-0.78629231 -0.90410423 -0.80814642 -0.83358091]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.34475422 -0.4832519  -0.16841075 -0.31661329]\n",
      "\taction [ 1.         -0.40704367 -0.41118065 -0.32513782]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 0.7693845  -0.24154694 -0.28735554 -0.32742587]\n",
      "episode 35 at 1000 ts; done reached\n",
      "\taction [-0.81927806 -0.93586516 -0.93928581 -0.8575086 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.05695791 -0.33369851 -0.2633062  -0.2223191 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.38404626 -0.48471922 -0.13241799 -0.28828639]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.3127203  -0.16793366 -0.25040764 -0.37498426]\n",
      "episode 36 at 1000 ts; done reached\n",
      "\taction [-0.87858731 -0.85815352 -0.88410902 -0.88500077]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.15775226 -0.29936081 -0.16929533 -0.42140552]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.216093   -0.34638712 -0.1946485  -0.13941912]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 0.42453501  0.04986176 -0.26317549 -0.45487833]\n",
      "episode 37 at 1000 ts; done reached\n",
      "\taction [-0.90233719 -0.86320847 -0.81709152 -0.71793944]\n",
      "\taction [-0.27563435 -0.40191844 -0.38809973 -0.20351152]\n",
      "\taction [-0.21593125 -0.54823136 -0.47231704 -0.47441596]\n",
      "\taction [-0.43941453 -0.31647992 -0.37590432 -0.10354295]\n",
      "episode 38 at 1000 ts; done reached\n",
      "\taction [-0.88499296 -0.81487584 -0.92291069 -0.87949276]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.67268467  0.25655335 -0.36900964 -0.23297055]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.32990059 -0.26530251  0.08195599  0.73714221]\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.26281238 -0.15825769 -0.29361278 -0.07738087]\n",
      "reward 0.03999999910593033\n",
      "episode 39 at 1000 ts; done reached\n",
      "\taction [-0.85302949 -0.92908299 -0.82368869  0.65933889]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.1998207  -0.13190152 -0.28669366 -0.38455856]\n",
      "\taction [-0.22917108 -0.38959223 -0.35770604 -0.24174508]\n",
      "\taction [-0.28713647 -0.36996284 -0.17647865 -0.37511411]\n",
      "episode 40 at 1000 ts; done reached\n",
      "\taction [-0.98060471 -0.90655404 -0.86678159  0.34424809]\n",
      "\taction [-0.39537278 -0.37083486 -0.26796189  0.96776581]\n",
      "\taction [-0.38675883 -0.3877573  -0.33172971 -0.54764122]\n",
      "\taction [-0.48251334 -0.34703642 -0.37271765 -0.2825318 ]\n",
      "episode 41 at 1000 ts; done reached\n",
      "\taction [-0.87775475 -0.84978366 -0.86540365  1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.16441718 -0.28984493 -0.31487411  0.7122097 ]\n",
      "\taction [-0.04000296 -0.31951413 -0.46967006 -0.34339026]\n",
      "\taction [-0.26396415 -0.14160125 -0.2189319  -0.18978171]\n",
      "episode 42 at 1000 ts; done reached\n",
      "\taction [-0.93096268 -0.80678928 -0.78225422  1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.25073609 -0.40974233 -0.33176139  1.        ]\n",
      "\taction [-0.29808751 -0.42756471 -0.27495864 -0.36299777]\n",
      "\taction [-0.58452362 -0.22271708 -0.34706849  1.        ]\n",
      "episode 43 at 1000 ts; done reached\n",
      "\taction [-0.91035742 -0.97631651 -0.813456    1.        ]\n",
      "\taction [ 0.35697299 -0.16940305 -0.20731939  1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.17957735  0.07337578 -0.38388467  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.25381857 -0.18844485 -0.28927982 -0.15191734]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "episode 44 at 1000 ts; done reached\n",
      "\taction [-0.66942966 -0.55712444 -0.97169697  1.        ]\n",
      "\taction [-0.31595531  0.51152891  0.17470263 -0.26017264]\n",
      "\taction [-0.40489957  0.04854296  0.92519325  0.94067389]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.34285986  0.39284277  0.04516744  1.        ]\n",
      "episode 45 at 1000 ts; done reached\n",
      "\taction [-0.84475619  0.09657876 -0.20981316  1.        ]\n",
      "\taction [-0.47735983  0.51376301  0.34129339  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.30178756  0.15119857  1.          1.        ]\n",
      "\taction [-0.14380956  1.          1.          1.        ]\n",
      "episode 46 at 1000 ts; done reached\n",
      "\taction [-0.83734202  0.40681067  1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.44620106 -0.0965137   0.86473924 -0.15578032]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.46353498  1.          1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.17975293  0.68882459  1.          1.        ]\n",
      "episode 47 at 1000 ts; done reached\n",
      "\taction [-0.92948061  0.74396509  1.          0.372318  ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.19172046  1.          1.          0.88500774]\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.45772883 -0.41338378  0.31872851 -0.36462712]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.13755305  0.8271091   1.         -0.31290716]\n",
      "episode 48 at 1000 ts; done reached\n",
      "\taction [-0.88252813 -0.09060287  1.         -0.4368425 ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.40888748 -0.26054752  1.         -0.19034334]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.38276726  1.          1.          0.52527696]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.33308929  1.          1.         -0.23119302]\n",
      "episode 49 at 1000 ts; done reached\n",
      "\taction [-0.93403095  0.95900017  1.          0.99061716]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.39647412 -0.22596492  1.          0.2305792 ]\n",
      "\taction [-0.50257516 -0.0141754   1.         -0.33753633]\n",
      "\taction [-0.29272634  0.93841767  1.         -0.13764563]\n",
      "episode 50 at 1000 ts; done reached\n",
      "\taction [-0.83337384  1.          1.          0.52681082]\n",
      "\taction [-0.28620714 -0.22134475  1.         -0.2442909 ]\n",
      "\taction [-0.15322833 -0.00426939 -0.2902945  -0.50935721]\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.28113487 -0.32541713 -0.18785153 -0.3094345 ]\n",
      "episode 51 at 1000 ts; done reached\n",
      "\taction [-0.86965132  0.09698365  0.9849667  -0.95219111]\n",
      "\taction [-0.48954898  1.          1.         -0.14650355]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.29470283 -0.31617507 -0.24346532 -0.45228556]\n",
      "\taction [-0.31497347 -0.13645245  1.         -0.25214046]\n",
      "episode 52 at 1000 ts; done reached\n",
      "\taction [-0.99506038 -0.67002016  1.         -0.59579021]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.29431459 -0.21634541  1.         -0.12380989]\n",
      "\taction [-0.38468853  0.93312007  1.         -0.11266606]\n",
      "\taction [-0.32442549 -0.51469034 -0.32728311 -0.29570401]\n",
      "episode 53 at 1000 ts; done reached\n",
      "\taction [-0.94440722  1.          1.         -0.95465237]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.27471039 -0.31609726 -0.3745811  -0.43267563]\n",
      "\taction [ 1.          1.          1.         -0.30683681]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.04963811  1.          1.          0.16106415]\n",
      "episode 54 at 1000 ts; done reached\n",
      "\taction [ 1.          0.22883098  1.         -0.88596362]\n",
      "\taction [-0.06321231 -0.43022332 -0.48619238 -0.35600862]\n",
      "\taction [ 1.         1.         1.        -0.2049575]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.          1.          1.         -0.31827378]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "episode 55 at 1000 ts; done reached\n",
      "\taction [ 0.59735787 -0.80754256  1.         -0.85139817]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.23072243 -0.41615999 -0.43640095 -0.22728416]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.38688841  1.         -0.33927256]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.15195541 -0.16661493  0.43061134 -0.32954291]\n",
      "episode 56 at 1000 ts; done reached\n",
      "\taction [ 0.85305905 -0.83504611  1.         -0.88339853]\n",
      "\taction [ 0.02635764 -0.47523397 -0.36960238 -0.30834937]\n",
      "\taction [ 1.         -0.21483952  1.         -0.29780647]\n",
      "\taction [-0.34023735 -0.09403537 -0.53379816 -0.3937524 ]\n",
      "episode 57 at 1000 ts; done reached\n",
      "\taction [ 0.38377646 -0.78786755  0.99471551 -0.71164399]\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.40931022 -0.31678993  1.         -0.49731418]\n",
      "\taction [ 0.98140419 -0.42681381 -0.50541806 -0.1643886 ]\n",
      "\taction [ 1.          1.          1.         -0.21685176]\n",
      "episode 58 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.808483    1.         -0.36305231]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.99807465 -0.25311619  1.         -0.46273834]\n",
      "\taction [-0.22764976 -0.29461142  1.         -0.3470341 ]\n",
      "\taction [-0.36614299 -0.14915073  1.         -0.36981958]\n",
      "episode 59 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.8747645   1.         -0.88949251]\n",
      "\taction [-0.36639169 -0.32494533  1.         -0.23170029]\n",
      "\taction [-0.46021384 -0.24226637  1.         -0.23787688]\n",
      "\taction [ 1.         -0.24864683  1.          1.        ]\n",
      "episode 60 at 1000 ts; done reached\n",
      "\taction [ 0.98006594 -0.07305014  1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.18218289 -0.19871724  1.          0.38834554]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 0.51591653 -0.327923    1.          1.        ]\n",
      "\taction [-0.30716971 -0.39583123  1.          1.        ]\n",
      "episode 61 at 1000 ts; done reached\n",
      "\taction [ 0.32229856 -0.89211053  1.          1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.28367496 -0.52590889  1.          1.        ]\n",
      "\taction [-0.25968513 -0.580746    1.          1.        ]\n",
      "\taction [-0.33022669 -0.18200579  1.          1.        ]\n",
      "episode 62 at 1000 ts; done reached\n",
      "\taction [ 1.        -0.8557182  1.         1.       ]\n",
      "\taction [ 1.         -0.35676268  1.          1.        ]\n",
      "\taction [-0.20438446 -0.44318408  1.          1.        ]\n",
      "\taction [-0.42734468 -0.26915085  1.          1.        ]\n",
      "episode 63 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.87353539  1.          1.        ]\n",
      "\taction [-0.28269815 -0.36442521  1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.47071886 -0.30644032  1.          1.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 64 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.82186192  1.          0.97559279]\n",
      "\taction [-0.12599619 -0.14567697  1.         -0.07647684]\n",
      "\taction [-0.32616675 -0.43748653  1.         -0.22416925]\n",
      "\taction [ 1.         -0.29996821  1.          1.        ]\n",
      "episode 65 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97403359  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.28481475  1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.24414118 -0.33175138  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.38210621 -0.22620986  1.        ]\n",
      "episode 66 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90136784  0.22967185  1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 0.31551656 -0.2644828  -0.25470361  1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.40260699  1.          1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.38774222  1.          1.        ]\n",
      "episode 67 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97072119 -0.60228312  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.19411184 -0.51167732 -0.20340642  1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.54734981 -0.25795478  0.98516333]\n",
      "\taction [ 1.         -0.46037003 -0.39343312  1.        ]\n",
      "episode 68 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.82255685 -0.92826229  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.30783671 -0.27058253 -0.25216115  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 0.31599942 -0.39860472 -0.38077083 -0.35729167]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.09594472  1.         -0.41830873]\n",
      "episode 69 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.89901578 -0.82016975  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.39297187 -0.32444289  1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.12762271 -0.46139055 -0.49202666  1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.12477288 -0.26974356  1.        ]\n",
      "episode 70 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97865951 -0.53895974  1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.26504394 -0.46465695 -0.31966814 -0.34677359]\n",
      "\taction [-0.1398143  -0.25085303 -0.52393401 -0.30652124]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.17205846  1.          1.        ]\n",
      "episode 71 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.99340773 -0.83948278 -0.1848117 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.34503371 -0.23472516  0.49876967]\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taction [ 1.         -0.24898377 -0.17247614  0.83556139]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 0.99303728 -0.4296588  -0.33367157 -0.37339109]\n",
      "episode 72 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.86681598  1.          0.96406686]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.1305849  -0.42486614 -0.47023281 -0.39599559]\n",
      "\taction [ 1.         -0.26014793 -0.20290124 -0.18944462]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.31019068  1.         -0.24228561]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "episode 73 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.98822874  1.         -0.92447662]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.28230777 -0.37981668 -0.39032295 -0.17154011]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.49366274  1.         -0.31097996]\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.22857617 -0.38333389 -0.24581558 -0.31273216]\n",
      "episode 74 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.99852151 -0.49722853 -0.70635712]\n",
      "\taction [-0.02364143 -0.49155024  0.12630211 -0.28566274]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.27313346 -0.01679607  0.68231851 -0.0829621 ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.43594295 -0.15378498 -0.30056608 -0.4357096 ]\n",
      "episode 75 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.8619833   1.         -0.81676316]\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.46673116  0.26037663 -0.25885805]\n",
      "\taction [-0.23709466 -0.25735483 -0.30767855 -0.24626063]\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.30055892 -0.39219478 -0.19585854 -0.26101664]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "episode 76 at 1000 ts; done reached\n",
      "\taction [-0.73439348 -0.91061836 -0.83004987 -0.89945   ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.18339887 -0.30004755 -0.36339775 -0.31823677]\n",
      "\taction [-0.47400463 -0.55048454 -0.14405179 -0.54949588]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.44255435  1.         -0.44991305]\n",
      "episode 77 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.99732471  1.         -0.94014162]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.1979966   0.30376482  0.56921071]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.44716591 -0.42480558 -0.40727249 -0.17804392]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.30131316 -0.39313775 -0.24977535 -0.32083699]\n",
      "episode 78 at 1000 ts; done reached\n",
      "\taction [ 0.15210502 -0.92600936 -0.84844559 -0.06075161]\n",
      "\taction [ 1.         -0.35479286  1.         -0.24097377]\n",
      "\taction [ 1.         -0.31654462  1.         -0.36992788]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.39878604 -0.53525496  0.38185826 -0.19290809]\n",
      "episode 79 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.84389967  1.         -0.83866793]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.40389231 -0.33563846 -0.05510284 -0.3218399 ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.35796094  1.         -0.10591025]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.27121583  1.         -0.22870348]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "episode 80 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.91910201  1.         -0.84807372]\n",
      "\taction [ 1.         -0.32501361  1.         -0.34905398]\n",
      "\taction [-0.21703917 -0.28184122  1.         -0.35748982]\n",
      "\taction [ 1.         -0.52670604  1.         -0.4015924 ]\n",
      "episode 81 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.84216082  1.         -0.32115477]\n",
      "\taction [ 1.         -0.28520873  1.         -0.25257954]\n",
      "\taction [-0.10601452 -0.44419792  1.         -0.33968151]\n",
      "\taction [-0.20401424 -0.25580147  1.         -0.2706072 ]\n",
      "episode 82 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.89494526  1.         -0.97029471]\n",
      "\taction [-0.28600943 -0.21417399  1.         -0.42174712]\n",
      "\taction [ 1.         -0.35185087  1.         -0.29826537]\n",
      "\taction [-0.38341576 -0.28526226  1.         -0.39284027]\n",
      "episode 83 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.8516736   1.         -0.95315456]\n",
      "\taction [-0.30176601 -0.39977434  1.         -0.52991456]\n",
      "\taction [-0.38813123 -0.40171394  1.         -0.40331018]\n",
      "\taction [ 1.         -0.36398593  1.         -0.11768513]\n",
      "episode 84 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.85349399  1.         -0.82390398]\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.27325094 -0.46930087  1.         -0.31304097]\n",
      "\taction [ 1.         -0.39219496  1.         -0.32922   ]\n",
      "\taction [ 1.         -0.14987606  1.         -0.23211478]\n",
      "episode 85 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.95692915  1.         -0.83090353]\n",
      "\taction [ 1.         -0.0552813   1.         -0.21497944]\n",
      "\taction [-0.43382668 -0.37602016  1.         -0.42118877]\n",
      "\taction [ 1.         -0.28733304  1.         -0.1592747 ]\n",
      "episode 86 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.92540193  1.         -0.86950362]\n",
      "\taction [-0.17248242 -0.29972425  1.         -0.23488057]\n",
      "\taction [ 1.         -0.44178179  1.         -0.34281591]\n",
      "\taction [-0.36878791 -0.15542214  1.         -0.40092555]\n",
      "episode 87 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.89762467  1.         -0.8495928 ]\n",
      "\taction [-0.26396024 -0.24116561  1.         -0.2588993 ]\n",
      "\taction [-0.35652572 -0.41874784  1.         -0.40525836]\n",
      "\taction [-0.33611184 -0.39122039  1.         -0.33840272]\n",
      "episode 88 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.95472133  1.         -0.33264485]\n",
      "\taction [ 1.         -0.31486979  1.         -0.23054406]\n",
      "\taction [-0.41415468 -0.34520692  1.         -0.20134012]\n",
      "\taction [ 1.         -0.314421    1.         -0.27455962]\n",
      "episode 89 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.89575255  1.         -0.98161584]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.29013994 -0.30359983  1.         -0.26387453]\n",
      "\taction [ 1.         -0.28760368  1.         -0.33100289]\n",
      "\taction [-0.23609658 -0.34793395  1.         -0.36914513]\n",
      "episode 90 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.81396556  1.         -0.86194766]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.0823417  -0.38612348  1.         -0.11606049]\n",
      "\taction [ 1.         -0.1778021   1.         -0.12006176]\n",
      "\taction [-0.30384359 -0.26495469  1.         -0.18567537]\n",
      "episode 91 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97346288  1.         -0.85670066]\n",
      "\taction [-0.45184594 -0.46181685  1.          1.        ]\n",
      "\taction [-0.16544893 -0.36949411  1.          1.        ]\n",
      "\taction [ 1.         -0.25296208  1.         -0.13059147]\n",
      "episode 92 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.83593678  1.          0.56179261]\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.40702295  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.26429874  1.         -0.44954377]\n",
      "\taction [ 1.         -0.31242982  1.         -0.42599881]\n",
      "episode 93 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.96458799  1.         -0.33249933]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.38052636  1.         -0.23154564]\n",
      "\taction [-0.54759771 -0.12015814  1.          1.        ]\n",
      "\taction [-0.37066439 -0.34692788  1.          1.        ]\n",
      "episode 94 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.82707638  1.          0.6763947 ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.26822391  1.         -0.34238112]\n",
      "\taction [ 1.         -0.08032975  1.         -0.38813093]\n",
      "\taction [-0.13475622 -0.42690906  1.          1.        ]\n",
      "episode 95 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.89809477  1.         -0.89968449]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.39322856 -0.29514191  1.          1.        ]\n",
      "\taction [-0.21794002 -0.3649784   1.          1.        ]\n",
      "\taction [ 1.        -0.2881504  1.         1.       ]\n",
      "episode 96 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97895259  1.          0.81642103]\n",
      "\taction [ 1.         -0.41862053  1.          1.        ]\n",
      "\taction [-0.35978404 -0.47729856  1.          1.        ]\n",
      "\taction [ 1.         -0.20552957  1.         -0.33144954]\n",
      "episode 97 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.95308441  1.         -0.50704688]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.40098548 -0.3139475   1.          1.        ]\n",
      "\taction [-0.37748218 -0.43228456  1.          1.        ]\n",
      "\taction [ 1.         -0.48660415  1.         -0.28212994]\n",
      "episode 98 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.93111283  1.         -0.98153776]\n",
      "\taction [-0.42500666 -0.19054838  1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.42217964  1.         -0.42539656]\n",
      "\taction [-0.32045013 -0.49148899  1.          1.        ]\n",
      "episode 99 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.9089514   1.         -0.83296078]\n",
      "\taction [-0.43346888 -0.48556617  1.          1.        ]\n",
      "\taction [-0.46000522 -0.40226707  1.          1.        ]\n",
      "\taction [ 1.         -0.36740726  1.         -0.39998132]\n",
      "episode 100 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.96537733  1.         -0.97623271]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.43620005 -0.19803782  1.          1.        ]\n",
      "\taction [ 1.         -0.49414486  1.         -0.37223208]\n",
      "\taction [-0.35897794 -0.35648084  1.          1.        ]\n",
      "episode 101 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.91694784  1.         -0.92764968]\n",
      "\taction [-0.22520065 -0.35438234  1.          1.        ]\n",
      "\taction [ 1.         -0.14909379  1.         -0.57628733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taction [-0.32345694 -0.33232018  1.          1.        ]\n",
      "episode 102 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.89376491  1.         -0.94327599]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.54493088 -0.37126434  1.          1.        ]\n",
      "\taction [-0.24586225 -0.29386619  1.          1.        ]\n",
      "\taction [-0.23543106 -0.18045758  1.          1.        ]\n",
      "episode 103 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.93088609  1.         -0.92775661]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.41693744  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.24854344  1.         -0.4073939 ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.52391577  1.         -0.40867537]\n",
      "episode 104 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.96533984  1.         -0.94119734]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.38095632  1.         -0.27370697]\n",
      "\taction [ 1.         -0.62336302  1.         -0.39782897]\n",
      "\taction [-0.34172335 -0.24496928  1.          1.        ]\n",
      "episode 105 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.84092563  1.         -0.8203339 ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.52284735 -0.17546389  1.          1.        ]\n",
      "\taction [-0.40262049 -0.30227935  1.          1.        ]\n",
      "\taction [-0.36657095 -0.51500815  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "episode 106 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.91879481  1.         -0.93758643]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.27898681  1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.37143499 -0.36905441  1.          1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.37098077  1.         -0.42158347]\n",
      "episode 107 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.92499799  1.         -0.98438966]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.14863186 -0.26097766  1.          1.        ]\n",
      "\taction [-0.29018238 -0.22002125  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.47075436  1.         -0.39770573]\n",
      "episode 108 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.9589169   1.         -0.85775775]\n",
      "\taction [ 1.         -0.33951062  1.         -0.32794622]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.31361872 -0.03582814  1.          1.        ]\n",
      "\taction [-0.24288371 -0.50723344  1.          1.        ]\n",
      "episode 109 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90594143  1.         -0.91748524]\n",
      "\taction [-0.24673021 -0.23273852  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.26531956  1.         -0.32625297]\n",
      "\taction [-0.34611669 -0.28722239  1.          1.        ]\n",
      "episode 110 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.9593448   1.         -0.52384686]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.40012965 -0.21537128  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.         -0.42524233  1.          1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 0.37930068 -0.487075    1.          1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "episode 111 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.94559979  1.         -0.87843204]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.10679018 -0.360773    1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.         -0.14879683  1.          1.        ]\n",
      "\taction [-0.29330498 -0.5890435   1.          1.        ]\n",
      "episode 112 at 1000 ts; done reached\n",
      "\taction [ 1.        -0.8801502  1.        -0.9802165]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.34577844 -0.42908508  1.          1.        ]\n",
      "\taction [-0.43809822 -0.31090394  1.          1.        ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.21004222  1.         -0.48436144]\n",
      "episode 113 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.99024093  1.         -0.78225148]\n",
      "\taction [-0.26263943 -0.22829111  1.          1.        ]\n",
      "\taction [ 1.         -0.27697581  1.         -0.42366436]\n",
      "\taction [ 1.         -0.29534963  1.          1.        ]\n",
      "episode 114 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90764683  1.         -0.39548442]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.39307719 -0.4825238   1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.46689266 -0.47834113  1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.         -0.26866949  1.         -0.45801356]\n",
      "episode 115 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.96570325  1.         -0.47162664]\n",
      "\taction [ 1.         -0.28975305  1.         -0.2123922 ]\n",
      "\taction [-0.23830295 -0.46295118  1.          1.        ]\n",
      "\taction [ 1.         -0.34715518  1.          1.        ]\n",
      "episode 116 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97330767  1.         -0.8971135 ]\n",
      "\taction [-0.37049761 -0.19582884  1.          1.        ]\n",
      "\taction [ 1.         -0.44597396  1.         -0.10614119]\n",
      "\taction [-0.20817156 -0.43973339  1.          1.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 117 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90658492  1.         -0.95718235]\n",
      "\taction [ 1.         -0.33430687  1.         -0.39117235]\n",
      "\taction [ 1.         -0.22204015  1.         -0.2867488 ]\n",
      "\taction [ 1.         -0.40123847  1.          1.        ]\n",
      "episode 118 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90262371  1.         -0.82792944]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.38181856 -0.37133914  1.          1.        ]\n",
      "\taction [ 1.         -0.34055457  1.         -0.30491441]\n",
      "\taction [ 1.         -0.43758392  1.         -0.39622566]\n",
      "episode 119 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.81698042  1.         -0.9842636 ]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.         -0.16133288  1.         -0.28901187]\n",
      "\taction [ 1.         -0.37700829  1.         -0.37239212]\n",
      "\taction [-0.0887683  -0.27033123  1.          1.        ]\n",
      "episode 120 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.85140008  1.         -0.80396682]\n",
      "\taction [ 1.         -0.34844783  1.         -0.4371047 ]\n",
      "\taction [ 1.         -0.19973433  1.         -0.21495329]\n",
      "\taction [ 1.        -0.2693781  1.         1.       ]\n",
      "episode 121 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.8868041   1.         -0.98583543]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.37284714 -0.28621519  1.          1.        ]\n",
      "\taction [ 1.         -0.19095118  1.          1.        ]\n",
      "\taction [-0.37460387 -0.45127192  1.          1.        ]\n",
      "episode 122 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.84559852  1.         -0.91319454]\n",
      "\taction [ 1.         -0.53405184  1.         -0.28905499]\n",
      "\taction [ 1.         -0.23056769  1.         -0.31271425]\n",
      "\taction [ 1.         -0.23505253  1.         -0.34753489]\n",
      "episode 123 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90615451  1.         -0.83130568]\n",
      "\taction [-0.02341466 -0.31151724  1.          1.        ]\n",
      "\taction [-0.32870448 -0.32570249  1.          1.        ]\n",
      "\taction [ 1.         -0.31895071  1.         -0.32183233]\n",
      "episode 124 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.97427851  1.         -0.80445719]\n",
      "\taction [ 1.         -0.39177677  1.          1.        ]\n",
      "\taction [ 1.        -0.3153342  1.        -0.2045923]\n",
      "\taction [ 1.         -0.4570848   1.         -0.39629877]\n",
      "episode 125 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.99400526  1.         -0.85832292]\n",
      "\taction [ 1.         -0.39734209  1.         -0.36801043]\n",
      "\taction [ 1.         -0.33635083  1.          1.        ]\n",
      "\taction [ 1.         -0.34694296  1.         -0.27461144]\n",
      "episode 126 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.84860867  1.         -0.93432027]\n",
      "\taction [-0.46387497 -0.39973608  1.         -0.30614531]\n",
      "\taction [ 1.         -0.3189995   1.         -0.55444503]\n",
      "\taction [-0.18612751 -0.20770164  1.         -0.43112752]\n",
      "episode 127 at 1000 ts; done reached\n",
      "\taction [ 1.         -0.90646869  1.         -0.84271646]\n",
      "\taction [-0.33307663 -0.48421723  1.         -0.22997269]\n",
      "\taction [-0.39922988 -0.3245092   1.         -0.0960668 ]\n",
      "\taction [ 1.          1.          1.         -0.20404612]\n",
      "episode 128 at 1000 ts; done reached\n",
      "\taction [ 1.          0.75244862  1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.2900441   1.          1.         -0.29575562]\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.35676184  1.          1.         -0.39786837]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 129 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.         -0.98390204]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.46769643  1.          1.         -0.50067848]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.          1.          1.         -0.46124193]\n",
      "episode 130 at 1000 ts; done reached\n",
      "\taction [ 0.48686212  1.          1.          1.        ]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "reward 0.029999999329447746\n",
      "reward 0.019999999552965164\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 131 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.07357738  1.          1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.08657043  1.          1.          0.66438687]\n",
      "\taction [-0.36011562  1.          1.          1.        ]\n",
      "episode 132 at 1000 ts; done reached\n",
      "\taction [-0.62308878  1.          1.          1.        ]\n",
      "\taction [ 1.          1.          1.         -0.39346012]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 133 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.20716113  1.          1.         -0.25676444]\n",
      "reward 0.019999999552965164\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.47648129  1.          1.         -0.24638344]\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 134 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.         -0.94698095]\n",
      "\taction [-0.28366533  1.          1.         -0.39647713]\n",
      "\taction [-0.26661623  1.          1.         -0.53034264]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.          1.          1.         -0.27391395]\n",
      "episode 135 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.34197411  1.          1.          0.54707408]\n",
      "\taction [-0.1683315   1.          1.         -0.26318613]\n",
      "episode 136 at 1000 ts; done reached\n",
      "\taction [ 0.10449251  1.          1.          1.        ]\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [-0.06934514  1.          1.          1.        ]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 137 at 1000 ts; done reached\n",
      "\taction [ 1.         1.         1.        -0.8597039]\n",
      "\taction [-0.0533923  1.         1.         1.       ]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 138 at 1000 ts; done reached\n",
      "\taction [-0.95427936  1.          1.          1.        ]\n",
      "\taction [-0.19049214  1.          1.         -0.40089637]\n",
      "\taction [-0.28754431  1.          1.          0.21736418]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "episode 139 at 1000 ts; done reached\n",
      "\taction [-0.93624741  1.          1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.          1.          1.         -0.37980145]\n",
      "\taction [-0.10346507  1.          1.         -0.31402564]\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.27279875  1.          1.          1.        ]\n",
      "episode 140 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.         -0.76156515]\n",
      "\taction [-0.23795477  1.          1.         -0.2967214 ]\n",
      "\taction [ 1.          1.          1.         -0.45188841]\n",
      "\taction [-0.57723463  1.          1.         -0.24444547]\n",
      "episode 141 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.36278695]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 142 at 1000 ts; done reached\n",
      "\taction [ 1.         1.         1.         0.6668328]\n",
      "\taction [-0.42556757  1.          1.         -0.28562966]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 0.39259279  1.          1.         -0.32902598]\n",
      "episode 143 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.         -0.43712348]\n",
      "\taction [ 1.          1.          1.          0.19322403]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.59931624]\n",
      "episode 144 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.          0.66978139]\n",
      "\taction [ 0.39268327  1.          1.         -0.3675406 ]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.33465123  1.          1.         -0.33554968]\n",
      "episode 145 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.24199139  1.          1.         -0.18720597]\n",
      "\taction [-0.46583411  1.          1.         -0.16907454]\n",
      "\taction [ 0.44765261  1.          1.         -0.2513147 ]\n",
      "episode 146 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 0.8184393   1.          1.         -0.26060247]\n",
      "\taction [-0.22088704  1.          1.         -0.25299352]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 147 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.          0.91932297]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.          1.          1.         -0.33311826]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 148 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.28879255]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 149 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.26123679]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.29906103  1.          1.         -0.38506383]\n",
      "episode 150 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.17154674]\n",
      "\taction [ 0.53787553  1.          1.         -0.32426637]\n",
      "\taction [ 1.          1.          1.         -0.32061556]\n",
      "episode 151 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.         -0.69489056]\n",
      "\taction [-0.211436    1.          1.         -0.42862728]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 152 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.41531289]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 153 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.          0.67382765]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.30423075]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 154 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.         1.         1.        -0.3836107]\n",
      "\taction [ 1.          1.          1.         -0.61406201]\n",
      "episode 155 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.42054459  1.          1.         -0.42003179]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.29096824  1.          1.         -0.23930123]\n",
      "\taction [ 1.          1.          1.         -0.17086886]\n",
      "episode 156 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.25843814]\n",
      "\taction [ 1.          1.          1.         -0.35663289]\n",
      "episode 157 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.36977455  1.          1.         -0.35352135]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.36986801]\n",
      "episode 158 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.42034289]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 159 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.37759638]\n",
      "\taction [ 1.          1.          1.         -0.28157973]\n",
      "episode 160 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.06457647  1.          1.         -0.29968572]\n",
      "\taction [-0.30142725  1.          1.         -0.36141855]\n",
      "episode 161 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.29230151]\n",
      "\taction [-0.30626798  1.          1.         -0.21149729]\n",
      "episode 162 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.          1.          1.         -0.40620634]\n",
      "\taction [-0.5110507   1.          1.         -0.28015077]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 163 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.          1.          1.         -0.20306638]\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.          1.          1.          0.00274413]\n",
      "\taction [ 1.          1.          1.         -0.26803359]\n",
      "episode 164 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.          0.93220603]\n",
      "reward 0.009999999776482582\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.45027646  1.          1.         -0.4276669 ]\n",
      "episode 165 at 1000 ts; done reached\n",
      "\taction [ 1.          1.          1.         -0.89001817]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.3595733  1.         1.        -0.2028995]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [-0.34510005  1.          1.         -0.45350653]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "\taction [ 1.          1.          1.         -0.40872017]\n",
      "episode 166 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "\taction [-0.44936073  1.          1.         -0.42511114]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.44160646  1.          1.         -0.62090665]\n",
      "episode 167 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.11317906  1.          1.         -0.37640914]\n",
      "episode 168 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.1568667   1.          1.         -0.53211862]\n",
      "\taction [-0.3171224   1.          1.         -0.28777587]\n",
      "episode 169 at 1000 ts; done reached\n",
      "\taction [ 0.96250939  1.          1.          1.        ]\n",
      "reward 0.009999999776482582\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.03999999910593033\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 170 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.24156089  1.          1.         -0.19646418]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 171 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 172 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 173 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 174 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 175 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 176 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 177 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 178 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 179 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 180 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 181 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 182 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 183 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 184 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 185 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 186 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 187 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 188 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 189 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 190 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 191 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 192 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 193 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 194 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "reward 0.029999999329447746\n",
      "reward 0.03999999910593033\n",
      "reward 0.029999999329447746\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 195 at 1000 ts; done reached\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.3037104  1.         1.         1.       ]\n",
      "\taction [ 0.98441857  1.          1.          1.        ]\n",
      "episode 196 at 1000 ts; done reached\n",
      "\taction [-0.897623  1.        1.        1.      ]\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.43303084  1.          1.          1.        ]\n",
      "\taction [-0.30164883  1.          1.          1.        ]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 197 at 1000 ts; done reached\n",
      "\taction [-0.82887071  1.          1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "reward 0.03999999910593033\n",
      "reward 0.009999999776482582\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "\taction [-0.21259241  1.          1.          1.        ]\n",
      "episode 198 at 1000 ts; done reached\n",
      "\taction [-0.99288452  1.          1.          1.        ]\n",
      "reward 0.019999999552965164\n",
      "\taction [-0.30211481  1.          1.          1.        ]\n",
      "\taction [-0.37207609  1.          1.          1.        ]\n",
      "\taction [ 1.  1.  1.  1.]\n",
      "episode 199 at 1000 ts; done reached\n",
      "[0.0, 0.029999999329447746, 0.5999999865889549, 0.03999999910593033, 0.0, 0.0, 0.0, 0.14999999664723873, 0.0, 0.0, 0.09999999776482582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06999999843537807, 0.3799999915063381, 0.6199999861419201, 0.4899999890476465, 1.099999975413084, 1.4299999680370092, 1.6799999624490738, 1.2599999718368053, 1.289999971166253, 1.1999999731779099, 1.3799999691545963, 0.7599999830126762, 0.3399999924004078, 0.3299999926239252, 1.1599999740719795, 1.6299999635666609, 0.2899999935179949, 1.389999968931079, 0.06999999843537807, 0.8999999798834324, 1.2599999718368053, 0.9199999794363976, 0.0, 0.9399999789893627, 0.3499999921768904, 0.0, 0.24999999441206455, 0.06999999843537807, 0.7099999841302633, 0.1699999962002039, 0.3499999921768904, 0.2799999937415123, 0.5299999881535769, 1.0599999763071537, 0.42999999038875103, 0.18999999575316906, 0.549999987706542, 0.1699999962002039, 0.40999999083578587, 0.4599999897181988, 1.149999974295497, 0.0, 0.12999999709427357, 0.19999999552965164, 0.0, 0.3499999921768904, 0.13999999687075615, 0.0, 0.03999999910593033, 0.0, 1.4899999666959047, 1.7899999599903822, 1.8499999586492777, 0.2799999937415123, 1.4999999664723873, 0.6099999863654375, 1.4199999682605267, 1.9099999573081732, 1.2299999725073576, 0.8299999814480543, 0.4599999897181988, 0.909999979659915, 1.579999964684248, 0.5799999870359898, 1.0599999763071537, 0.0, 0.0, 0.0, 0.0, 0.03999999910593033, 0.0, 0.0, 0.0, 0.0, 0.11999999731779099, 0.06999999843537807, 0.0, 0.18999999575316906, 0.07999999821186066, 0.2699999939650297, 0.12999999709427357, 0.0, 0.09999999776482582, 0.09999999776482582, 0.0, 0.1099999975413084, 0.0, 0.1099999975413084, 0.3999999910593033, 0.1099999975413084, 0.2699999939650297, 0.5999999865889549, 0.8099999818950891, 0.09999999776482582, 0.2199999950826168, 1.649999963119626, 0.3399999924004078, 0.12999999709427357, 0.0, 0.2799999937415123, 0.0, 0.0, 0.0, 0.08999999798834324, 0.09999999776482582, 0.0, 0.13999999687075615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13999999687075615, 0.6399999856948853, 0.41999999061226845, 0.3499999921768904, 0.0, 0.20999999530613422, 0.09999999776482582, 0.2199999950826168, 0.1699999962002039, 0.17999999597668648, 0.06999999843537807, 0.40999999083578587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04999999888241291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07999999821186066, 0.0, 0.0, 0.03999999910593033, 0.0, 0.0, 0.0, 0.0, 0.11999999731779099, 0.1099999975413084, 0.3799999915063381, 1.2299999725073576, 1.149999974295497, 0.06999999843537807, 0.7199999839067459, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09999999776482582, 0.0, 0.019999999552965164, 0.06999999843537807, 0.019999999552965164]\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "from collections import deque\n",
    "\n",
    "print_every = 300\n",
    "# seems to be 1000 for the env anyway\n",
    "max_ts = 2000\n",
    "max_episodes = 200\n",
    "curr_agent = agent.Agent(state_size, action_size)\n",
    "\n",
    "\n",
    "# can't use a queue.Queue because \"Insertion will block once this size has been reached, until queue items are consumed\"\n",
    "scores = deque(maxlen=100)                          # initialize the score (for each agent)\n",
    "scores_history = []\n",
    "\n",
    "\n",
    "# env expecting the list?\n",
    "# action not in the form env was expecting?\n",
    "\n",
    "# dropout too high?\n",
    "\n",
    "\n",
    "episode_won_i = 0\n",
    "\n",
    "with active_session():\n",
    "    for i in range(max_episodes):\n",
    "        # initialize for the start of the episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        state = env_info.vector_observations[0]                  # get the current state (for each agent)\n",
    "        # resets the noise class variable\n",
    "        curr_agent.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_ts):\n",
    "            action = curr_agent.act(state.astype('float32', copy=False))\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('\\taction', action)\n",
    "                \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            reward = env_info.rewards[0]\n",
    "            if reward != 0:\n",
    "                print(\"reward\", reward)\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            #score = score * curr_agent.Q_DISCOUNT + reward[0]\n",
    "            #score = score * curr_agent.Q_DISCOUNT + reward\n",
    "            score = score + reward\n",
    "\n",
    "            curr_agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # done is a vector\n",
    "            if done: \n",
    "            #if done[0]: \n",
    "                print(\"episode {} at {} ts; done reached\".format(i, t))\n",
    "                break\n",
    "        scores_history.append(score)\n",
    "        scores.append(score)\n",
    "        if i % print_every == 0:\n",
    "            print(\"episode {}; average score past 100 episodes: {}\".format(i, np.mean(scores)))\n",
    "        if np.mean(scores) >= 30:\n",
    "            episode_won_i = i\n",
    "            print(\"Solved in {} episodes\".format(episode_won_i))\n",
    "            curr_agent.save()\n",
    "            break\n",
    "            \n",
    "    print(scores_history)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10/26:\n",
    "#scores_history = [0.19999999552965164, 1.2999999709427357, 0.7399999834597111, 1.0599999763071537, 0.8399999812245369, 0.8999999798834324, 0.5899999868124723, 0.9499999787658453, 0.1099999975413084, 0.1099999975413084, 0.2799999937415123, 0.5099999886006117, 0.0, 0.0, 0.0, 0.1699999962002039, 0.0, 0.2899999935179949, 1.9499999564141035, 0.14999999664723873, 0.6799999848008156, 0.35999999195337296, 0.6799999848008156, 0.7099999841302633, 0.5299999881535769, 0.7699999827891588, 0.5999999865889549, 0.47999998927116394, 0.29999999329447746, 0.8999999798834324, 0.789999982342124, 0.5399999879300594, 0.40999999083578587, 1.649999963119626, 0.5799999870359898, 0.909999979659915, 0.25999999418854713, 0.4399999901652336, 0.35999999195337296, 0.5899999868124723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11999999731779099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05999999865889549, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13999999687075615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12999999709427357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12999999709427357, 0.0, 0.0, 0.0, 0.0, 0.1699999962002039, 0.0, 0.09999999776482582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09999999776482582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11999999731779099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13999999687075615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11999999731779099, 0.0, 0.13999999687075615, 0.11999999731779099, 0.0, 0.0, 0.0, 0.13999999687075615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09999999776482582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1599999964237213, 0.0, 0.14999999664723873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04999999888241291, 0.0, 0.0, 0.0, 0.08999999798834324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07999999821186066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17999999597668648, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12999999709427357, 0.0, 0.0, 0.08999999798834324, 0.0, 0.0, 0.0, 0.08999999798834324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09999999776482582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12999999709427357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09999999776482582, 0.0, 0.0, 0.14999999664723873, 0.0, 0.0, 0.11999999731779099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05999999865889549, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1599999964237213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019999999552965164, 0.0, 0.0, 0.1699999962002039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12999999709427357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07999999821186066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03999999910593033, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f21ac75f6a0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXmcJFd1JvqdiMilsqq6qltV6m71om6JRgsCCdEIsDC7hYAxMl7GkjewwWI8xtvM+D3w84Ox/OY33gaPPcBgjS1jmzEyxoBlRiyyhVglrBba1VpaLbW6eu+urjW3WO7748a9cWPLjMyKzMrMvt/v17+uyozMupkZee4X3/nOOcQYg4aGhobGuQNjvRegoaGhodFf6MCvoaGhcY5BB34NDQ2Ncww68GtoaGicY9CBX0NDQ+Mcgw78GhoaGucY2gZ+ItpBRF8nov1E9DgR/VrCMUREf0pEB4joESK6Wrnv3UT0jP/v3Xm/AA0NDQ2NzkDtfPxEtBXAVsbY94loEsADAH6EMfaEcszbAfwKgLcDeBWAP2GMvYqINgHYB2AvAOY/9hWMsbM9eTUaGhoaGm3RlvEzxo4xxr7v/7wMYD+AbZHDbgDw14zjPgDT/obxVgB3Mcbm/WB/F4Drc30FGhoaGhodwerkYCLaBeDlAL4XuWsbgMPK73P+bWm3t8TMzAzbtWtXJ0vT0NDQOKfxwAMPnGaMzWY5NnPgJ6IJAP8A4NcZY0vRuxMewlrcnvT8NwO4GQB27tyJffv2ZV2ahoaGxjkPIjqU9dhMrh4iKoAH/f/NGPt8wiFzAHYov28HcLTF7TEwxm5ljO1ljO2dnc20aWloaGhodIEsrh4C8BcA9jPGPppy2B0Afs5397wawCJj7BiArwK4jog2EtFGANf5t2loaGhorBOySD3XAvhZAI8S0UP+bb8FYCcAMMY+CeBOcEfPAQBVAD/v3zdPRL8L4H7/cbcwxubzW76GhoaGRqdoG/gZY99GslavHsMA/HLKfbcBuK2r1WloaGho5A5duauhoaFxjkEHfg0NDY1zDDrwa2hoaJxj0IFfYyDQcFx8dt9h6FGgGhq9hw78GgOBbz19Gv/X5x7BE8eitYEaGhp5Qwd+jYHAatMBANRtb51XoqEx+tCBX2Mg0HR4wHdcHfg1NHoNHfg1BgINP/Dbrtb4NTR6DR34NQYCMvB7mvFraPQaOvBr9A0vnKlipeEk3iekHtvRgV9Do9fQgV+jL2g4Lt7xp9/Cn3/rYOr9gJZ6NDT6AR34NfqCR+YWsdxwsFxPZvxC6nG01KOh0XPowK/RF3zv4BkAgOslM/qGb+NsaqlHQ6Pn0IFfoy+47yDvxp1Wmdt0udTjpGwMGhoa+UEHfo2ew3Y9PHDoLAAgLa4Lxm9rH7+GRs+hA79Gz/HI3CJqNmf0bgrj1z5+DY3+QQd+jZ7jwRc42y8XjFSpJ3D1aMavodFrtJ3ARUS3Afg3AE4yxq5IuP83Afy08nyXAZj1xy4+D2AZgAvAYYztzWvhGsODM6tNFEzC9FgxNbmrWzZoaPQPWRj/pwBcn3YnY+wPGWNXMcauAvAhAN+IzNV9o3+/DvrnKKoNB+MlCwa10Pj9wN/UUo+GRs/RNvAzxr4JIOuA9JsAfGZNK9IYOaw0XIwXLRgGwUuzczo6uauh0S/kpvETUQX8yuAflJsZgK8R0QNEdHNef0tjuLDacDBeMmEQwUuzc2qpR0Ojb2ir8XeAHwbwnYjMcy1j7CgRnQ/gLiJ60r+CiMHfGG4GgJ07d+a4LI31xmqTSz22a7eQenTLBg2NfiFPV8+NiMg8jLGj/v8nAXwBwDVpD2aM3coY28sY2zs7O5vjsjTWG6sNB+NFC0RZ7Jya8Wto9Bq5BH4imgLwegD/qNw2TkST4mcA1wF4LI+/pzFcWG24UupJtXPqAi4Njb4hi53zMwDeAGCGiOYAfARAAQAYY5/0D3sXgK8xxlaVh24G8AUiEn/nbxljX8lv6RrDAiH1mETpdk5XaPxa6tHQ6DXaBn7G2E0ZjvkUuO1Tve0ggCu7XZjG6ECVetJbNnCNv6kZv4ZGz6ErdzV6jtWmyxm/0ULqcTTj19DoF3TgzxlnV5v4yD8+Jl0q5zps10PT8TBe5Bp/ktTjekx25dQav4ZG76EDf8749oHT+Kt7D+HJY8vrvZSBwKo/arFV5a7ag19LPRoavYcO/DljqW4DAOq2ZvwAl3kAYKLkV+4mSD3q1ZGWejQ0eg8d+HOGGC1Y15OkAASMv9KicrehvFejLvW4HsMn7jkg3xcNjfWADvw5Y1kz/hBWFKknzc6pSj32iE/gevL4Ev7gK0/h2wdOr/dSNM5h6MCfMyTj7zDwH56v4q4nTvRiSeuKaoO/D63snKrUY4/4lZLY+NLqGTQ0+gEd+HOGCPyiEjUr/ua+Q/j12x/sxZLWFQHjN1PtnHX/vTINguPpwK+h0WvowJ8zpNTToZ2z2nRGskFZtekH/qKVaucUGv9EyRrJ90CFyHGkdSnV0OgHdODPGUtdSj0N2xvJYKDaOdOknmYo8I824xevXzN+jfWEDvw5I9D4OwtgDcdL7Vw5zFhpBHZOs42dc7xkjnzgFwFfx32N9YQO/DmjW1dPw3HBGFJbGgwrqk0HBvFB6+3snOMtpJ6Ty3U8f3o18b5hgphAljaJTEOjH9CBP2cs1UTg75zxA6MnAazIBm3EA3/C29LIIPX8/pefwgc+8/1eLrUvkFLPiG3wGsMFHfhzBGNMulg6Te4KF9CoBYRqw0WlZAKA37Ih3cffKvAv1W2s1Ie/6El8vqO2wWsMF3TgzxGrTVcyunqzc6kHAEYs7mPF78UPoIXUIzR+K7Vlg+16spHbMENKPaP2QWsMFXTgzxFC3we6YPwjKvWsNhxM+IHfNFLsnHbA+B2PJeY5HJeNhC7uacavMQDQgT9HLCtSRNca/4gxwWrDRaXIpR6i5Csa0ZFTbBBJCd7miDB+XcClMQhoG/iJ6DYiOklEifNyiegNRLRIRA/5/z6s3Hc9ET1FRAeI6IN5LnwQEWL8Hbp6xPGjwGpVrEQYf6LUYweuHiC5UZvteiMRLMVL0FKPxnoiC+P/FIDr2xzzLcbYVf6/WwCAiEwAHwfwNgCXA7iJiC5fy2IHHaJ4a7xodmHnHFGpp+mgUgw0/qQrmobjwjIIJYufjkk6v+OykWD8gdSzzgvROKfRNvAzxr4JYL6L574GwAHG2EHGWBPA7QBu6OJ5hgZC6pmdLHUu9fgbxahJPasNN5zcTbFzliwDBT/wJw1jGRXG7+rkrsYAIC+N/zVE9DARfZmIXuLftg3AYeWYOf+2kYWQemYnS10nd0ctHvDkbtzO+W//7F58/OsHAHA7Z6lgomAQgFGXenQBl8b6I4/A/30AFzLGrgTwPwB80b+dEo5NPduJ6GYi2kdE+06dOpXDsvqPpVrA+DvpzskYG0mpx/MYaraLsWLcznnw1IqsxG04LoqmgYKZLvXYLhuJ90ZKPaO2w2sMFdYc+BljS4yxFf/nOwEUiGgGnOHvUA7dDuBoi+e5lTG2lzG2d3Z2dq3LWhcs121YBmG6UuxI41eljVEIbgJCky+anAMYBklt2/WCQN5wPJQKBiz/uDSpZxRaNouXphm/xnpizYGfiLYQEfk/X+M/5xkA9wPYQ0S7iagI4EYAd6z17w0ylusOJssWylZnyV119OAoab/itRi+hGNQ0IvI9ZicttWwucZfFIw/IcDbLoM3Ar2MNOPXGARY7Q4gos8AeAOAGSKaA/ARAAUAYIx9EsCPA/glInIA1ADcyPi30yGiDwD4KgATwG2Mscd78ioGBMt1G5PlAsoFo6OZu6osNEqMXwZ+zgtCdk6PAa4f4Juuh5JlwvIDv+0kST2BFCauDIYRnvTxr/NCNM5ptA38jLGb2tz/MQAfS7nvTgB3dre04YNk/AWTM1rXk7p1K6ijB0co7svXYpJg/BQqYBJafsNxUbQMFPyAbicyfn6b4zFYZq9X3ju4cuMboQ9aY+igK3dzRBD4+duaVe4ZValHBHk/7ocqd12maPwRqSdp7q7YJIb9ikgPYtEYBOjAnyOWpNTDKWlWL//ISj3+azF9jd9UCrg8VeP3ffyW1PjD7wFjTCZ8h72Iy/NGYwPTGG7owJ8jVpu8PUHZEoE/K+MPjhulgBDV+A1F4+eM39f4HS8k9URdPWqwH/b3R6x/2JPUGsMNHfhzRK3JG5KVfKmnkbGIa2Slnpirh6Qzh7FAvrFdD0XLlPmQqNSj+vqHPfBrV4/GIEAH/hyx6nei7FjqcUZT6hGxLUjucqlDvEbB5G3Pg2VQUMAVeQ9Gqc5B9+rRGATowJ8TRJVqpWgpgT8j47fDrp6nTyzjY3c/05N19hMiSPuEX9o5BdsVAd5xGSyDpE0z2rJB/X3Yi7hkd84h38A0hhs68OeEmh+8K0UTZUu4eoIg1XQ8/P5XnsRizY49Nir1fPnR4/ijrz2dOoZwWBAt4CJf6gksnapFU3H1RFo2jJLUI1+7lno01hE68OeEqj9qsVJKZvyPzC3gf97zLO599kzssVGpRwTEYQ9ygpwbitQDBIHdkf97mRn/8L8nukmbxvpDB/6cUG3yBm2VgqLxK8ndM6tNAMmdJ9UNwvPYyBT5iPWLGjah9TsRa6bj8mrcoEnbCAd+pY5BQ2O9oAN/ThCMf7xkKgVcQcCabxH4Q4yfBQNHhj3IuQl2TiBg/GqSt2AaKBiiH3/4davSz7D7+F02Gp+txnBDB/6cIBj/WEpyt3XgDyd3AzmgZ8vtC1g08FNYynG84H/TIBSsc0jq0YxfYx2hA39OkBp/0Uws4DqzwgN/M6EdgVq563kK4x/y4OCmaPzy9bkMjDHYLkPByCb1DDvj9zTj1xgA6MCfE1YbQeAPCrhUqacBIC5jRI9zPTYyrDCq8UcZv+0xqXlbpgHLEJW76VLPsAfMIH+zzgvROKehA39OqNl+crdooWQZIIow/oxSj6v43Ifd+RE0aYtq/IFrSfxsGgQiQsGk0U7ualePxgBAB/6cIBj/eNEEEaFkGckaf5LU44SlnlHxekvGn2rn9KR0I/r0WIZxThRwDftnqzHc0IE/J9R8jX+syPX9sYKJmu3i/ufnwRhrndy1w64ed0RcPWL5hrBzGmE7p+sxuP4mYPkHFUyKFXCNlNQzIp+txnBDB/6cECR3+WybcsHEPz18DD/xyXtx77NnpNSTrPGHXT3OiLh6gpYNQeUuEPTesT0mh66I4q2C2Y7xD3fA9EakRkNjuNE28BPRbUR0kogeS7n/p4noEf/fd4noSuW+54noUSJ6iIj25bnwQUO16aBkGZLVlgumbM+w79BZ6eZJ8/EL77+nJHeHXQ5gMrmbLPWoU7gCxm+EWjTw48NS2DBDu3o0BgFZGP+nAFzf4v7nALyeMfYyAL8L4NbI/W9kjF3FGNvb3RKHA9Wmi/FSMMmy5PfrIQK+91zQpiEt8IsrBddLLuA6s9KQctKwIMr4o5W7anJXMH7LpATGP0IFXP5LG/arOY3hRtvAzxj7JoD5Fvd/lzF21v/1PgDbc1rbUGG16WCsEAyDnZko4SUXbMArL9yEBw6dlbcna/yufKzLmGSF6rCOG2+9D//9n5/u1fJ7AqnxpxRwAUFiW1g5i6YRH8QyQq4e8ZkO+9WcxnAjb43/vQC+rPzOAHyNiB4goptz/lsDhVrTxXgpCPx/cuNV+PR7X4U9mydCrRsaKa6eip8U9hT5Qw0Ox5fqOL5U79Xye4JgAhf/PdqyAQgsr2LsomVSS6ln+Bm/lno01h9W+0OygYjeCB74X6vcfC1j7CgRnQ/gLiJ60r+CSHr8zQBuBoCdO3fmtay+YbXpYqwYvJ3nTZQAAHvOn5C3FU0j5lgBIoGfJevAtaYrE8jDAi9F41ctmSKxXTDSk7tqQnzYNX434WpOY+3wPD6XuaxcdWukIxfGT0QvA/DnAG5gjElBmzF21P//JIAvALgm7TkYY7cyxvYyxvbOzs7msay+otZ0UEk46fZsnpQ/n7+hlOLjd6UNVLVzivho+3530Q9oWBAt4BIbgO2ojN8L3VdoI/UMO+MX8V5LPfniL7/7PN78374x9MSgX1hz4CeinQA+D+BnGWNPK7ePE9Gk+BnAdQASnUGjgNVGWOoREIy/ZBmYGiuk+vhFcjepV49g+sPG+OXoRSNs57QVxi+kHtGnp2QZMTksXLk73FnRQOpZ54WMGI4u1HBkoYYX5qvrvZShQFuph4g+A+ANAGaIaA7ARwAUAIAx9kkAHwZwHoBP+F9sx3fwbAbwBf82C8DfMsa+0oPXMBCo2WGpR2B2soQNZQsTJQtFK85mAS71yOSux2JSj3DzDK+rB6H/1asewfiFq2esaMpiN4HmKLl6RqQdx6BBnGv7jy1h18z4Oq9m8NE28DPGbmpz//sAvC/h9oMArow/YjSx2nAwXowzfiLCJVsm0XS8RP0a8F09UuMPkrtiAxBjHYeN8QfJ3Yid01OlHv6aZP2DZcZmFWtXj0Y7iO/V/mNLeNtLt67zagYfuSV3z3XUmkHwjuKWG66A7Xr4g688JYO4CjW5qzJ+wQqFtj/sgT9auQsEU8qE1DNWNGPv0Sg1aQvyN8P9OgYN4n194tjSOq9kOKADfw5gjGG16WA8QeoBgMu2bgDA+9As1cOMX7gRxhRXT1TjFwy4NmTJXS+i8Qe9eoKgJ/oUWbLi2QjZXwFu/zQIoUHtwwqxh2nGny+EW27/seV1XslwQPfqyQENx4PHkMr4BQqmERvEIthvpeAndxmLTeCSyV3bHSoboAjSQT9+/r+dwPhFy4ZywUS9GWf8Igcy7Bo/0y0begKR9D+yUMNi1V7n1Qw+dODPAXLebpvAn5TcFYx3rMg/CjfB1SOSuowhxoYHGULqoUjlbkjqaYoCrqDHkTqkHuCBv6wkv4cZOrnbG6iEYP9xLfe0gw78OUBo8JUUqUegmFicxH9XA1tUB1Y17357+Y8s1PBzt/0rluuds6hYP/4kqce/AhL9+McKJmyXhb37LpPvT7Sqd9ggP9vhfhkDB8dl2DReBMATvBqtoQN/DpAtmRN8/CoKphEqXgKCQFAw+dQuj6XbOdW/1S88OreAbz59Cs+f7twfnTZzNyT1SFePkHr4/3VFEmu6nmx6N+zauC7g6g0cj2HLhjIAYEFLPW2hA38OUAett0LBSuo8GVSumkTczplSwAUg0RXUSzQT+gZlhXT1iEEssklbvHJXJHeFlq9udrbroWjxmbyjUsClpZ584XgeCibBNEjPOsgAHfhzQLXB5ZexQmupJ6kdQcD4CYZBcL34XNaw1NPfwK+2UO4UYv1RO6fTws5Z8gO/6uV3XAbL/1IPe3LX0z7+nsD1mCRPw36O9AM68OcAmdxtI/UkafyiYZlpGDHGL87fsNTTX43fXkvgT7FzJks9YcavTiVrurz4zTRIjmocVuhBLL2B7Xqw/HNEX021hw78OaBqZ5R6Erpzit8LBsEgv4Ar6upR2G+/2zao07I6hStdPfx3qfF7camnoLh6AKDWDDYH2/VQMPzAP+RMWUs9vYHrMVjGaFwV9gM68OcAwVpLVvvAr7p2ANXrLqQehfHLyt31k3oEO+9GN2URV49s0ubEGb/oxx8kd8NST8EiX+Mf7i+1WP6wb2CDBttlkvEP+znSD+jAnwNEUZZwnqShYMWlDvGzkDKSXD1125XJz35LPcI+2Q2Lio1eTJJ6nGzJXf7+GEPP5oJ2HOu8kBGDyvh14G8PHfhzgBq8W6Ho368meFXGn+7qcXDeRNH/ud+uHp/xr0HjF/796LB1gDeoA9SWDfHkbtNlsAzf1TPkGr8b+Ww18oHtekHg1+9tW+jAnwNE4C+2YfziflXqEEHQUlw98QIuD5vG+USvvAL/Pz18FMcX249ydNag8QeuHvj/p8/cNSOBvxZy9XgoWoF+e+DkCr7+5MmO1zMIEG+jthzmC9fznV80/OSgH9CBPwfIBG0bxi/uVxmvWsBlEnckxJK7TQfTYwUYlE9y13Y9/OrtD+Lv9x1ue6xwHXUjscRHL8bbMteaXMYS+r/Q+Bt2JLmrSGF/8e2D+NDnH+14PYMA8dkypscv5glu5zR0cjcjdODPAc1I24E0BIFfCWpewHgN4sE+Zue0XVSKJipFKxfG33A8MJY8+D2K5hqSu2LjkpW7/tkWZvyu7NMDKBq/rWr8gdTjeAy1pps412AYoMoQWovOD7bnoWDoAq6s0G2ZcwD3mQesNQ1iYwhp/NLOacDwPcjxfvwuykXT71W/9uSu2KiyBM98pJ7wIJZo5W7BCPhHksZvK1KP63lw2PB26VSDksuY/gLmBNflBVyWZvyZkInxE9FtRHSSiBJn5hLHnxLRASJ6hIiuVu57NxE94/97d14LHyTY/nStdigmMH5HYfyCrcjkrnD1NF1UCiYqRTMnxs+fI2kMZBRrsXNGC7goQeOvOy5MhfEnafw8cedfxruMt8Ee0i+3um7t7MkPtsftnII8abRGVqnnUwCub3H/2wDs8f/dDOB/AgARbQKf0fsqANcA+AgRbex2sYMK0UumHaTUozRqC/IDQbm5iLEi2Fb90YxjBROrjbUH/k4Yv1hfN10xozN3g0Eswd9lLOjFL44pmuFhLI7LQhp/w3GHltW5EcavkQ+EnZMzfr2jtkOmwM8Y+yaA+RaH3ADgrxnHfQCmiWgrgLcCuIsxNs8YOwvgLrTeQIYSTT8wtUPBam3nNIxwEze1O+dY0cR4ycpX6nHaBx7ZsqHLAi4itR+/eM7wc0VzI3wKV6RlgxVcxjdsb2j1cTUmDetrGETwlg0EgwhDmv7pK/JK7m4DoFpE5vzb0m6PgYhuJqJ9RLTv1KlTOS2rP7BdT8o4rSACXFoBl0EITehyGa/ybTh8AlV+Uk8nGn/3Pn6XManvA8l2TiC4EhAoF8yYxi9bNvjvx7CyZVUy05JEfpCM3xz+Dq79QF6BPymryVrcHr+RsVsZY3sZY3tnZ2dzWlZ/0HS8to4eQCngclIYP1GIDXsekwGw4ks9edg5xRVHNo2/+8pdjwUJXUAZxOJ3UhSIXi2pA9f58Hl+jGUYcFyGpsMZ/zDaIaPJXY184PgtGwwiaBt/e+QV+OcA7FB+3w7gaIvbRwrCZ94OyXbOoIDLNCgUjD0WFGzlyfg70/jXkNz1GFSjk5R6nPAVkhVl/FbA+OUVkUUwDPiM3/XX1PGS1h3qW64Zf35wPE9q/Jrxt0degf8OAD/nu3teDWCRMXYMwFcBXEdEG/2k7nX+bSOFrMldWbkbsnP6gc2XMqIavwiAY0ULYzn5+IPA34HG32UBl8rsBftv+uX10aSvQLloomaHN6eCwRm/y5iUqoYxiecxJjc6HffzgedfFVq+JVrnTtojk42YiD4D4A0AZohoDtypUwAAxtgnAdwJ4O0ADgCoAvh5/755IvpdAPf7T3ULY6xVkngokTm5K3v1BCemkFBMU0g9KuNnMcZfy6FJW0caf8Ra2glcDyGNn5TK3XKBYJmGL5OF37uyFSR3HdX1JJK7jsg7dLykdYfH+LnieK6WenKCo1w1W0Z8yp1GHJkCP2Pspjb3MwC/nHLfbQBu63xpw4OodJEG6eNXNH5x0haMhOSux5RB7r7UY7u+W4bg+MMnOoX4G80slbvO2hi/SuYFsxfTkiyD0ARClbsAT+4uVJsAgs3JMoPRi6KxG2f8rVthDxpETxnYWurJC9FamLqt39d20C0bcoCwG7ZDUltm4ZoRJ22ojw9jMslZLvDKXcZ4teujc4u4/MNfxbHFWhfrDevnrRDtFNoJolKPEdL7Sd4X1fjHCkFyV+Q8ima4gAsYTjuk5wVXh8O4/kGEZPy6LXNm6MCfA9aU3HWDk9YgCrFwz+9LA/iM369qrTYdzJ2toul6OLHU6Hi93Wj83bVlZolSDwCYRhDw1QIuQPj4fR1fSD1WkPxei/y03uAOJf/KR0s9ucBVvkMm6bbMWaBbheSAZkapJ0njdz0uhxhG3NXjekHrgjG/SRvAnT4i+GWRa5LWC3RYudutxq8md5WfLcOA56s0UalHtXNKqcdPfqt21mEM/C4LGL+WevKBaHRoKVeFGq2hA38OsF1PVuW2QlKvHtsLdPqoqyea3BWuIEcZ39hNIktIJR316umySZuRYOcEeKdOiyVLPaWQnTNI7loGYbURJLeHkdmFpJ4hXP8gwo1IPbo7Z3vowJ8DbJd1VLkbSuC6gb3PIAolfj0W2DnLBVNJjnprYvzdVO52rfEnVO4C3NrJ/LcsmqAeKwaBX3j2SwUTpmGE7KzDyOy4qydIcmusHeI8kIxfv69toQN/DshauWsaBKJod0418MeHtIjAXrQMeRxn/NlZe9J6gay9etYg9fjuIwEjUsUrTr5Yrx7LhO0yOK4ntf6SFfTjFxg2ZsdYUIUMDKcddRARTe5qCa09dODPAVkLuIgIBdMIBWsnIvU0I1KPtHuagQvGcdnaNH63i8rdLr5MjIV1/Wgxl2GKIexRxs9/rztewPgtM5QvAIavJ79Yrvi8h23jGlSIq1JLqfXQaA0d+HNAM6OrBwBKphFi2k5E6lHhKlq+ZRgyCequUeNvdqHxd1MT47bQ+EO9ehKatAF8GIuQpcoFI5YLGDapRAT6onb15IoQ4yfN+LNAB/4ckLU7J8BbM0elnoLC+FW4nupqIcmMHc+TuuZaAn8nE7i6G8TCQiw9KvuIX5O6cwK8HbXQ+kuWmfD+DNcXXN3EAe3qyQviHDV9cqQZf3toH38OsDO2bAC4ZBMt4JLDyCOBzfPbMgu7p6VIPe5apJ4OfPziqqCbvjhRHz8QBHnTICl5RJO7IvA3nNFi/ExKPTq5myccL5B6DNKunizQgX+NELJL9sAf1vhtUcKPZKlHDBoHwi0PpMbfhbOlKeUb1jb4BMVSHf8ZeF64LTMQyD3qRhZN7sqB682gPUPJMkMjGsX6hwmulHq0nTNPqFKPnrmbDTrwrxGCvWdJ7gL8Sx9y7igaf9QY5DHm9+MJB8iQq6crO2d4yEkaPGVj6HoQixEN/MFrVdm/inKBv5c1ReMvFYzYJjKwpP4GAAAgAElEQVRsX3Ap9fifo3b15ANp59TdOTNDB/41QrDnLHZOfpwRadLmSUafJPWoQ0uExq8y/rVo/O0eb3vhXESnYJEmbYAS+EOMP+LqSUruWmZM6hm2S3oxOEYXcOULdUO1dODPBB341wjb6YzxFyyK+/iFrTFB6nG8wDEU9vGvvXKXPz79SxKaBtZFkHI9FmPzag9+K/K6BGRy1+bJXSJhZw2/x8NQwHXno8dw95MnAAQBqqhbNuQK2bLBn1utA397aFfPGhG0FOhO41ftnEmuHjFLVL1/rZW7WRm/o9zXXVvmsJMHCK5qVMYf685ZDDP+smWCiGI9fYbhC/6xuw9g43gBb7p0s2T4OrmbL1xF6tGMPxs0418jRBDtKPBnkHoM4izbVjaGJMbfVeWu8phWG0eoYVzXLRvCtwmpJ9SWOfLeVYqiCyln/CVf848lv4dAKlms2ZIciOUWdAFXrlBdPbo7ZzbowL9GNLtK7kYYv3T1BMcVTEO6bqyIz99xmZQ5esv4w0noTsGtqC3snDLwp7l6XDRszviB+JXBMMxWXag2Y+MrdeDPF+GWDQYY0zJaO2SKVkR0PRE9RUQHiOiDCff/MRE95P97mogWlPtc5b478lz8IEC6ejImd4tW2NXjqIFdCZJFy/AZvxfrW6+6erpN7oo/1VrjXzvjj7t6+P8mZdP4G07A+JOksEFG0/Gw2nTlBqp2keS/r9vSRgpBARdBXDxq1t8abTV+IjIBfBzADwGYA3A/Ed3BGHtCHMMY+w3l+F8B8HLlKWqMsavyW/JgQQ4D77aAywsCuxoki6bh2zmV5K/UhtfenXO8aGGl4bR29ajJ3a7aMvP2yyqE5m+ENP7wQSXLABHX+Ou2h5IlCr2Gi/Ev1mwAwTkipR5Lu3ryhKNcSanOt8JwTeXsK7JEq2sAHGCMHWSMNQHcDuCGFsffBOAzeSxuGNB54DdCrppQcldh/ELq4XZO/tyFBI0/S/VtFE3Hw3iJfyta5QicNdo5o6MXgeA1morGH7XCEhEfv9jkjF9cAQwb41+shecGuxE7p5Yj8kF4fCm/TSd4WyNLtNoG4LDy+5x/WwxEdCGA3QDuVm4uE9E+IrqPiH4k7Y8Q0c3+cftOnTqVYVmDgUaHyd1ywZTVqEDEzmlEpB4Pvp0z6upRK3e7S+6Ol/jFnt3iikFtJteVnTOhZUPIzhmpT1AhBsurjD9ewDXYkX+hyhl/dFRkQfkcNdYOqfGbaj8r/d62QpZolSRep72rNwL4HGPMVW7byRjbC+CnAPx3Iro46YGMsVsZY3sZY3tnZ2czLGswIBh31uRuuWCgHmL8gauHQoyfuxNUL7zU+HPo1TMhAn8rjV8JrN3aOWOBX5G1xIYXlXAAvkHW2zL+wf5yy8AvXT1+4NdST66QbZkNQ7rI9NVUa2SJVnMAdii/bwdwNOXYGxGReRhjR/3/DwK4B2H9f+ghC7iyMn5lrCAQHsSiPkXRMv1ePR4KolePGWf83SZ3x4si8Ldi/GuUerwWlbsUbGRJVc9jBVO2bIhq/GKTHfjA72v8zYjUIz5vHZzyQYjxmwHjf+bEssyzaISRJVrdD2APEe0moiJ4cI+5c4joEgAbAdyr3LaRiEr+zzMArgXwRPSxwwyp8VvZXD3lAg/8gv2Fkreqq8ck2Z1TMuOkCVwpjH9+tYkjC7XY7YyxkNTTWuMPKk27Su4mafwK44+2olAhBq7XbRclywwdN+77/Ac+8Fe5xi8YqbiAKmrGnytCM3f975DHGG76X/fhz791cD2XNrBoG/gZYw6ADwD4KoD9AD7LGHuciG4honcqh94E4HbGQmfzZQD2EdHDAL4O4PdUN9AooNlhcrdcMOCx8EhDM6GASyR3bZch2szMzdCP//e+vB+/9OkHUtc74Sd3WzF+cWypYHQVpJJ8/OJXS9H4o4NYAM74q02f8RfCts+Kf7Uy6IFTsM3oTIPAx78+6xo1BD7+8HjSxZqNJc34E5GpZQNj7E4Ad0Zu+3Dk9/+c8LjvAnjpGtY38Gh2KvWIBmSOi6JlhJO3EVfPatMNtXwW99uqxp+i0Z+t2phfbcZuF8noiXJ7qUcErJJldj16MZ7cVQq4pMafzPjnV5t+ctcMPbYyNIw/IvV4WurpBYLunCTJk+N6sF0GW7/HidCVu2tEp8ndktJ5EhBtmeOMn7t6uMavyiMGRVw9jppHV9flJcpA4rbA1dO+gKtcMLoetp5m5+QtG5ILuABE7JxRxj8kgb8WdvVEGf+gr39Y4Hi8IFGtDREExxl0z+86QTdpWyM69fGX/Q2iYfuVt56XOIilYPryCgsnPy3DyOTjt10vUb8XgX+i2F7jDwK/2aWrhyFC+EPjFtNaNgDR5K6v8ZsRqWfAA6fQ+F2PwfNYgtQz2OsfFjgekwYIQZ4EseqmzuVcgGb8a0QQ+LMnd4HgxAx35wyOK/mM31VyAAAPkrxyt3Vy13ZYNsafoXKX5yW6c/WkJneVbpvRyl0AKBe5xt90vFTGP+hebdVRYnueLDjT3TnzhTq+1JKBv/uWJucCdOBfIzpP7orA74H5g1YsI874Ld/Hz+2cwe2mP1quXT/+ZprUI5O7WTT+YAhKt1JPS42/BeOvFEwZOANXDz9ubFiknmoQ+B03YPx69GK+UIsgxflVU4iVRhw68K8RnSd3+XF1x1UmB4WbkBnEfw66c6pSD0U0/uTA7fj9fKIJxDjjz6Lxd5fc9byk5C7/3zTaaPzFQF6KVu6OD5HUI16+4wafhW7ZkC/Uq2YrIvUMenX3ekEH/jVCdM+MdqFMgyr1iOAdtWuaBsEg8pO7YanHjGj8aRq9SNpG7xdJr4ro1dOqZYMbBN6uu3NGC7hUqSdl9CIQvE/qz2IDHAbG73oMS3UHGytFAPxziA9i6c9aluo2rrrla7j32TP9+YN9RqjDbSTwp7neznXowL9G2C7LLPMAkL3l63bQYVPkB0iRQUwieIx79gtRxq/040+TasTtDSca+PkXomQZsU6hac/BGX/mlyiRVMAVSD1I7FEkMKYEfsn4/Q2wVDDkFdGgQvjHZydKADjzFMsVn2e/pJ5Tyw0sVG08eXypL3+v33CU1uXiXBLmCe3qSYYO/F3iG0+fwls++g0s1+3MiV1AkXpsVw43EQFN7VxpGFzjd9xw8DQN8hOFwiKYfHILph9l9OL3kmXywe8ZKndLBaOrS2bXi49eVF9jwPiTpR4Byfj940uWOfCTloSVc2aSM35V6jGI23L7JfWIIDiq7QtUOVR8V7TG3xraztklHjuyiAMnV7BpvJjZww+EpR7RBC3ovgn/f95e1vMYbGXYOiBcPSwUiG2XwYr0HhcBPSr1BIHf8AN/+hdDPbYb4sQYQ/RiSOwDhkHyfSua8cbplWKc8YurhZI1+IxfWDlnfMbfdIPN2vTbVfTLzinOgVEN/NwgkSz12FrjT8TIMv7/8NmH8Nn7D7c/sEuIS/lDZ1YzJ3YByPYDdScIBLKAKyL1RLtzivtUjR9I1ulFQI8xfmVUZHTwexRiSEy3QcplLNZKWbXdveWyzfi9H30pdmwaiz1W1fhlywYzHPgHmc1Jxi+kHsXVYxCB+njFItqAL1ZHNfB7sTxZTfr4deBPwsgG/nueOoV/fX6+Z8+/VOdfohNLDdlmNwtEQGvYrjwpo/qkaRhc6vGTu6qds2AYXONXA3/CyS06a6ZJPUXTQNGk1v34/fyFZRhdaaWex2JSjxy2bhAmywXceM3O2DFAWOOPtmUWgX+QC6BW6g4AYNM4l3ps1wsFftNP3vcDI8/4Q7UwYR//IJOD9cTIBv6m4/V0t1/yv9hAdg8/oCZ3g1msUQ+yaXANXKw/7OpJYPxdaPxFy0DBaq3x2y6vKjb8RHOn8Fg8cSsrdxOCvYqxBKlH1fgtgwbaqldt8vNjw1gBgAj8/D4h9fSLjI66xh8aZkTRyt3BPUfWEyMd+LsZUpIVate/TgJ/weSJPdXVo/biAfyBEgbJ9Yd8/LJyl0lWnMTaA40/3Mun4YSlnnY+/qJpwDS6HcQSt3NGL8nTkMT4d2ys4Dfe8mK86bLz/SuijpfUN1Sb/H3f4DfDUzdrg/i/fl2xiM98pAN/ROMX7rVBr+5eL4xkclf0nO9p4FcYf7EDVw8RyZ78jkzuhl09hsGPi9r/gDDjr/g966OsxvWYfGzUztmMBP6WGr8/K8A0jK4Cv+uxWH2DlHq6YPyGQfi1t+wBIArZBjfyi8A/FWL8fuCXjL9fUo+v8Y9q4E+wc9b897+VlHkuYyQZv0xs9pASLitfok5cPYA/jMUJpJ6ABfP7+SDy4PhQrx6p/XtBviBycqsbQVpyt2T5Gr//+zMnlvHbX3w0pDs3Xe4oMo3uPOft2jK3QhLjjz7PYDN+BwYhNOIypPEb/Uzu8jdqYVQDf8KwItmrRzP+RIxk4E/Tt/NEtxo/wDt0JhVwRV09wfNHGL/fj1+w4ijjV3+PSjkNJbmr+vjveeoUPn3fCzi92pDHOn5y16Tu2GmSq0fE+3aVzkkFXCqE5DWoqDZdjBctWVHquEGTNpOCyux+QP0+qGM/RwXq3GrZnVNIPYPMDtYRIxn4paOlp8ldWwakjgO/L/WIwGVG9Ekz0gJCZce8LTPfNITXPbrBqcE+KblbNA0QEQ/8fmsHIU3Um+qmEW5H0WmgclvO3O1E6okzfpNooPXbasPFWNGUm7atFnAZ6KvUIxg/MJpyT9J40rou4GqJTBGLiK4noqeI6AARfTDh/vcQ0Skiesj/9z7lvncT0TP+v3fnufg09Jrx123eLvjFmycBdB74SwUTdduTAVrYNQPGb4QZf8TVI5q3CQkkyupDUk8kuVtrOrJ6uGAFGr/wPdcURhjYOflaOgm0TNGzVURlrTSULEM6gJIY/6DbOau2i/GSJc+NqJ2zW6dUN1AJ0CgGftuNd7it2b0nf8OMtsldIjIBfBzADwGYA3A/Ed2RMDv37xhjH4g8dhOAjwDYC4ABeMB/7NlcVp+CZoqHPS8ID/8lWybx6JFFFDMOWhcoFwyf8Se7ekwDLRg/ydGLlRSpR33d0fdgue5gsswTjqrGX/Pth+HAz/sEScbfQaANHCzJds52yV0iwljBhOPGE8RAIHkNKqoNB2MFU9k0gyZt/Iquj64e5TMdxcDvKq4ewfwbtnb1tEIWqnoNgAOMsYOMsSaA2wHckPH53wrgLsbYvB/s7wJwfXdLzQ6xy/fKw7vs6/uX+Iy/k8pdgHv562oBV8TVY/p2ToGonVO8vrEukrtLdQeTvsVQ1fgl428GQcLx20WIdXUiTaiedRWycjeDE2qsYMqq3SgGnvE3XYyXTIXxh3v1dJs36Qbq+bEwgtW7tufJ6WxRH7/rMXn1qREgS8TaBkDtfTDn3xbFjxHRI0T0OSLa0eFjQUQ3E9E+Itp36tSpDMtKR88Zv8+aLpodR8GkLjR+I+Tqkcld6epBSBsP9eoxDOlRTk/uBid6dFNYrtuyqEj18UuNX2X8jrBzdi71iKAcJfZZ7ZwAz4Uk6fsA/AKuwf1CV5sOxopRqYffZxBkE75+QD0HRpfxJ1fuAnr8YhKyRKykb2j0nfwnALsYYy8D8M8A/qqDx/IbGbuVMbaXMbZ3dnY2w7LSkdagLC8IR890pYDrXrIFV+2c7ujxZV/jj/Xjp6CASw2M0V49IlknGH88uatq/HHGv0Fh/OKx9SSNXzD+LpK7IvDHXT3Z7JwA39iS9H0AsqXFoIK7ekx5ZSOcWIDSdruPjF9clY5i4OctG8IGiahkqRFGlgKuOQA7lN+3AziqHsAYUyc8/C8Av6889g2Rx97T6SI7hQhmUbabFwTj31Au4OM/dXXHj08r4JJDSoxwYIz24xevS1buRjX+FlLPct3GZNmXqKxA4xeMPyT1CDunv5ZOGGo0fyEgfm3n6gF4h860y3RrCAI/d/UEjF+g7wVcjofzJoo4tlgfzcDvxQu41CvXQc4FrReyMP77Aewhot1EVARwI4A71AOIaKvy6zsB7Pd//iqA64hoIxFtBHCdf1tP0WupR2j8IknaKXhy14sndxXGH2b5YVePOKmlnTMq9bRN7rbQ+BOSuzLwd6Hxt2rS1g6tpB5j0O2cTQfjRSts54y5evrF+PkmtKFshVqNjAqS+vGrpE+3Zo6jLeNnjDlE9AHwgG0CuI0x9jgR3QJgH2PsDgC/SkTvBOAAmAfwHv+x80T0u+CbBwDcwhjrXctMH71O7gpXz4ax7jpelCzT784p7JyRmbsGhaQetTunZQaMv9yhj58xhpVGNPDzY2sRjd/1GE4s1fGy7VPdJXfFphbV+COyVitcumVSbrJRWCaF/OmDhmrTRaVoSgmCFxIFSfy++vh9qWeqUpBzAkYJqp0zSULUjD+OTJGLMXYngDsjt31Y+flDAD6U8tjbANy2hjV2DBHsxHQqq8Pkazss1WxYBoWqSzuBaNkgC7gijgST4rp+0s+VTD7+4Odqk1tIxZWK2qtHMH0R+B86vICzVRuv3TMb2gyywkvx8UupJwPjv+WGK1LvG2TG73oMDcdDRWX8HoNhBAVcPLnbn/U0HQ+lAs83jKLU46ozdxMIhdb44xjJyt1mi+RmHliuO9gwVkjsI58F5QJn2oK5FxRtH4gXcFkRV4/AWCrjT5Z6AomK7/fCx88YCzR+P8jf/eQJmAbh9Xtm5Vo68vErsoaKaM1Ct7AG2M4pWjJXiibIHzFpu15s9GK/bIYNx0XJNDA9VhzJwC8qzIFkQqEDfxwjGfhb+djzwFLdlsGzG4iK25UGDxDRVsXRAq5oclegaBkwqLWdMxz4g6S0eDxjfHOsy+QuP/7uJ0/hFRduxFSlEDhTOqrc5f+nNmnrctMUMA1jYC/hxSZaKfmzgk3ivXoUp1M/ffyc8RuYGiuMZOBXNX7yN1UVg3pluJ4YycAfqlztwW6/VLNl8OwGZd+iuOoHfsHojZCdMzg+JPWEGrYZKFrx1srBABdCQ7lvKcL4heSzXHdQVZK7Rxdq2H9sCW++9PzQujqxHwaJ6/DtwczdzE+VCLOPla+dQgZ+/4pM5FKkj9/vf9RvjX/TeBEnlxsdX2mcXmnIvNaggTE+jS7cwTZ8cmnGH8doBv4WTcrywFLd6TqxCyiM3w/E0e6cwu4nEC7gCrN/1YsvIDaCStEM3Se+vCLgi9cwv9qUQahuu/jX53j+/XUv5vUU3dg5gwKuiNSTE+PnzeoGJ/DPrzZlglxs6JVi2D3lKU3rzD66egTjv/yCDViuOzh0ptrR43/xr/fh//tStEPLYCCYWx2cT1FSoQu44hjNwN/CzpgHlutrZPxS6uGBIir1WBFXTytrZ6kF458oWYkavyjgEq/hxFJdHlNrujjrOz+2bCiH/n4n0opw0MUKuDpo2dAKg1bAdeOt9+KP//lpAEGeRDB+S7TSZiz0Wfeb8V+5nRcaPjy30NHjTyzWcWq50f7AdYDY/ENtTSKRX7dmjmP0A39PpB5njYGfv+0rDc7ApZ1TaWcQYvxGa8YfnTIkfh+PBf4w45+UgT/4UtdsF0u1sCS0puRu5AzrpGVDKwxaAdfRhTqOLtQApDB+j3fnJOWqrl9EtOG4KFkmXrx5AuWCgYcOdxb4q7YbaoEwSHCSGH/k1NKMP46RDPyhQSRO/h/6WpO7JSW5SxSu2AUSGL+Zxv4JxYSB6eJEHy9ZoY0v6uoRUk+I8dsuluq2324gXF/QlZ0zZRBLFjtnKxh9TI62A2MMq01HBvxaTOMn2aRNtez2bRCLL/VYpoGXbpvCw50G/qYrB5sMGgSbV1l+9LzVGn8cIxn4w4w/3xPW85jfeXENGr8VSD0qm0/V+CMsP/g5eW5uU0o9ZozxmwbJgJQk9dRtlyevx4IrGtmrpxONP7Vlw+gx/mrTBWPAqi/dRZO7lmnICVzrKfUAwJXbp/HY0aXMwdD1mD+5azCDp3DGTSjfR3Fuifff0ZW7MYxm4FdO6rz79Qj9drzUXfEWoEg9fiAWUDX+8MzddMbPk7vJBVzjxbjGP1m2pNwggrsI/JMlC7UmZ/yqlNWVxp9m52zht+4Epjk4BVyC6YsgFPj4wxXSXOrhj+lnywbB+AHgqp3TaDoenjq+nOmx4rWoPf0HCTLwK1fgghwF8yoG4zwZJIxm4O9hcjeq33YDkdxdbbihpJTauVJ1w1hprh6TUu2clkEoF0w0XQ8f//oBvOsT3wn16QGA8aIJg4Djvsa/cbyImu3GjhPrSnL1/OFXn8R7P3V/7Pa0QSx5ST39dMW0gwg+q00R+JOkHq7xq32Z+rF+1+N2x6LJ1yISvI/MLWZ6fK0Z7+E0SBDOOJXxmzLw89sGtd5jPTGagb/FsPG1YtX/IkysQeoRjz2z2ggFclUGSBu2rm4CpkEYKxiy+EpAjEwsWtzq+diRRTz4wgKeO72KyVLA5IkIk+UCTvqMf9N4kUs99RSpJ2EPffLYMh49Eg8igcYfvt3MSerhE7gG4xJeSDyrcgPw5yUUFFePx5vySY2/T1csgvgIxj87WQIALNSy9exJmtMwSFhOYPzifC2ndK/VGNHA36o75VoRMP7upZ4dmyp4/+svguOxUPdJU2H8afJO1NUzVjBRtcONzERXzaJloOF4OLPKv+SPzC3EktIbxiyc9K16m8aLXOqpBT371b+fpJVWmy4WanasKEj2449q/HlJPQOk8S/77iyRPK81+dhF8VrVAi5xJTdZsiRb7SXE0B6h8YtZxrVmtkAeBP7BDJ7iPZxMYPzjKYOKNDI2aRs2hHv15MtUxBdhLcldAPjQ2y7DW1+yJfQFJH8bFm17BaLD1tWfK0ULtWa4IMd2PRQtA0XTQNNxcWaFB3aPxVtJbygXcNjjNsRNvtSz2EFyt+oPnq/bnuwdJP6WeC0q8mrZYBn9m2DVDoLxNxwPjuth1R+7KFAwDaw2He7q8T/KqUpBbpjd9nzKgkaE8YtZxtWMgb/mk4q64/Z8rd0gSeMX5+uYTO4OxnkySBjNwO94GC+aWG26+TP+5toZv8DVOzeGfg8mcAWMX7V7AvFClbGiGWNvtsMnEonirvnV4LJ+Q5TxKxvBpvEiPManNIWSu7Itc/w1CJlpodbEWHFM3i41/pTunGtt2TBIBVziKpD/7KLmD2ER4L16eHJXvJfTY8XEDTNviPNfnQtdKZqZNXuxqTHGN5Fylx1pe4VEjT/q6tGMP4bRlHpcTzLyZt4av/8lXyvjT4LauVJ8TwuRCBmt3K0UTdlnR8B2PRSsQOpZUBpzqUweQEj62VgpJt4e+PgTpB6fEUaHeLMUjb+T0YutMEh2zhUl8K/4fv5xJfkvWja4CmOervDPIavW3i2E1FNSAnYSWUiDemUwiPMPhMavvt/R5G7eMWAUMJKBv+F48tIvb8ZfbeQj9STBUBh/WoBM1PgjX+Kmy2flFk3efZMxPtQEQILGX/D/dniwTJLUk0ScRACJBn7ZpK1Xdk6D4LH+tTZuhTDjd1Czw4xfunq8wNUz7b+/0fctb0ipR5ldXClY0qbZDjUlfzSIRVwrdQcTJSt0ZRmTejTjjyFT4Cei64noKSI6QEQfTLj/PxDRE0T0CBH9CxFdqNznEtFD/r87oo/tBZqOJy/9eiX1jPfg8rxoGfjtd1yGd7xsa7AJRHraRDX+Mb8Rm8p+bZcX7BSVL/sb/U6bscDvSzqVohWSr8I+fv5/kqYuNp3FCHNNH73oP+daXT1dTAXrFdTAv9Jw/EHrqq+cN5TzWPD5TVXCgd/1GH799gfxWIJDai0QgV89F8rF7Bq/etwgOntWGnbMYScZf0Fr/GloS1uJyATwcQA/BD48/X4iuoMxprbrexDAXsZYlYh+CcAfAPhJ/74aY+yqnNfdErbryS9e7oxferR7kx553w9eBAAyAKidOYG4j18E65rtyi+AaucU+IGLzwMAXHf5ltDzCZZfLpihiWIq+09ry8wYk1pxlLmmuXrefOlmzK82sWm8iLXAVGYEpIzl7QpfeuQoDs/X8EtvuDjzY0SzPYBvAqsNJ/T6CqYhm7SJ/W56jN8vNszTKw188aGjuHTrBlyxbSqHV8Ih5Jkw4zczB3FVEhpEL/9KwwkldgFV6tGunjRkYfzXADjAGDvIGGsCuB3ADeoBjLGvM8aEteQ+ANvzXWZnaLo8YWYalPuHvtJwpFWyl0iVeqI+fn8DUi/dVTunwMxECf/39Zdi18x46PkmJeM3Q4k7lfHLubGRwN9wPDlwZaGWIvVE3qad51XwH6+7ZM3ukF4x/s9//wj+4tvPdfSYqNSzWLMxpUhlBZPQFFJPVOP3N0xZBNbI1+IpHG6hwN814x+8ALrsSz0qxHemaBk9iQGjgCy0dRuAw8rvcwBe1eL49wL4svJ7mYj2gQ9i/z3G2Bc7XmWHsB2GomnIL1yeqDacnrF9FeLkLbTU+A15OVtruvjyo8dQ9Z1MQuMXOC+FYQuXz1iE8Ycqd/2niTJ+NSikMf5e2f+6mRGQBfOrTZxeaaDpeJk395Wmw62zrofluoPTKw1ZKAUEE7jUyt0guesHft+dkjZcvluIVgtqvUi3yd3BlHqcmHwpNteCaciW2BphZIlgSd/cxHeSiH4GwF4Ar1du3skYO0pEFwG4m4geZYw9m/DYmwHcDAA7d+7MsKx0NF0PBeljz1vjd3ui70chYraZQeMH+Bf0U999Hos1G5WiifGSFQpcG9MCv89Mx4pmKCGZmNxl0cAfBKm4xp+c3M0Lck05f6nFLIITS3Xs2FTJ9JjVhoPZyRKOLNRwdKEO22WYnQgCv5R6vGAjHCuYKJqG3DBXe8z4i10y/pryGQ9k4K87cm6EgMiLWT750b164shCaeYA7FB+3w7gaPQgInoLgP8HwDsZY7LBO2PsqP//QQD3AHh50h9hjN3KGNvLGNs7Ozub+QUkoemI5KaZe5O2arKdBBMAAB4VSURBVNPpiaMnCiH1RO2cMVePovEv1R2cWW3C8bjGLy7vN5StWK5AQEg6rRh/ULkb/gLVWjF+L/w68obVQ8YPAMeVjqXtsNpwsHkDD/TPn1kFgBDjFx1UOePntxERpioFuWEuRxq95YUkjZ87wbL9nUGXelYacalHnHNFk/jVlu7OGUOWwH8/gD1EtJuIigBuBBBy5xDRywH8GXjQP6ncvpGISv7PMwCuBdDzGW5N10PR8qdT5d6ywUWlD4E/OpUrerv4WZV6lus2zq42fakn0PjPU9hnFCLAqxp/uWAktpLoROpJG8SSF0Q9Q54av+1LNQBwbDF74F9puNg0XoRlEJ47zQP/jPKe8149vIBL3Qinxwoxxp934E9i/GNFKzGI/939L2D/saXQbVXblefcoDL+aHJXkIKCyWcQaI0/jrZfS8aYA+ADAL4KYD+AzzLGHieiW4jonf5hfwhgAsDfR2yblwHYR0QPA/g6uMbf+8DvM37hn84TvDin91JPYOeMMv5wp86KTO7yrpqOx3B6pelr/HydrRw0IglZVqSe6HSxtEEsskV10Ywld4MCrl5JPUhc01ogZB4AOL5Yy/y4VZ91jpesVMbvegxLvgwnMF0pxJK7+TP+uMZfKfKurY7r4e/3Hcbh+Sps18NvfeExfPSup0OPrzYcbPTzEYMW+D2PYaXphPr0AAj1SCoYpKWeBGSiroyxOwHcGbntw8rPb0l53HcBvHQtC+wGIjFX7AXjb7qYrqzNipgFcjZtG8YfaPyODBpnVhshH3+rwC99/IrUE0uWRXr1PPjCWTQdT0o9W6fHsFgNa/xiv11roVYaesH4z64Gm1cnjH+1weW/iZKFI/74xbDGz9+DQ2eqsp4CAKbGivJ4caWRt8afWMDlnzPz1SZ+83OP4P2vuwg/8+oL4XoM3zlwGnXblVd/1Sa/mjm90hy4wF+1+QCcVMZvGShYhi7gSsBIVu7aonI1oVf9WsE1/j4kd1MKuMTvome/CPxnVpoyCDIW9OoH0h09QPClGVOknmhbh6jG/+F/fBy/+3+ekFLP1qlyjPGntWXOC4Lx51Gc8/jRRTz4wtlQT6PjHUk9gvHz969oGqE6CHHVdma1iW3TQT+j6UpBbphS6snZ1dNMKOAS58yxBf4aX5ivYu4s34CqTRffe25eHluzXdnKo54ziVorgj494fNVkKaiSbAMgq0LuGIYuSZtnhg80StXT2NtYxezQig68V49Ye1faPwnlsOBSrVztmL8pkF41e5NuOKCKTnDNyr1GIpnvm672H9sCZs3lGWC8IKpMXyreVoO9QbSZ+7mhTwZ/3+980mcXmng1968BwAwM1HMzPgd10PD4b2hxHkxO1kK2VjVxPr2jUrgHysEds5eST2OB4PCV47iyk68xkNnqvLKAwDu3n8CHmN40ewEqk0XF0zxNWe1gPYLK3477DTGL0aT2gO2YQ0CRi7wq8mspHm0a0W12R+N30wp4BIbgTi5BXs7udQIH5dR6gGAv3v/a+TPYwUzlfF7HsMTx5bgeAxnq02p8W+Z4na6xZqN8yf7FPhzLOA6vdLAoTNVObfgsq0b8MyJlUyPXVV6Nwl3ycxE+P1WB+ls2xhm/NWmi4bjhgJ/nu2Pm36LbvX5hNRzzM9jHJ6vYu4sr7/8wT0z+Jv7DuGv7j2EH335NtT8+dJFyxi4Xj3LCb34AcXHbxm+q0cz/ihGTuqRgd/MX+MXg9b7WcAV69VjhjeEkmXAoPDAdIBvfLOTJUyNFfCSC7K3ALhk86Rs6Cb/JgVSz8OHFwD4A1j8xOQF037gV5w9vdf48wv8YhM7cJIH+8u3bsDJ5XombXilKeQGU7YJURO7QDghv2NjUBswVRFtG2wpW3gs39YIDdsNJXYByGpvwfiXGw4eO7KI8ydLuPGVO1EumJidLOG5M6uoNh3u+LKMgevOmdSLH1CKH02SnVE1whg9xq9omiXLwJmEwM8Yw/3Pn8Urd23siFmJ9sf90PjTkrtW5HYi7uyJBv6CSZgaK+Dhj1zX0d/97L97Tew2wyAQcRYvAj8QBI4tvhSg6vxB5W5Hfz4zrJwCP2NMJnUfPLyAiZKFnedV4DHg1EoDW6fGWj5ebdOtSj0qBOM3KLg6AoIOnYtVWzb/A3hAy4tcCMavQjD+o4q8872D89izeQLveNlWvP2lW/BbX3gUX3v8BCc6JZ7/GbTkblIvfkBp2WAaKBiGrtxNwMgxfrG7t0ru3v/8WfzbP7sX+w6d7ei5qzkMWs8KM8XOGVwJKN0WC2ai1JP3elyP4eG5RZk7OLZYQ7lgYJPPXFUvv/D895zxr7GAa7XpynNk/9ElTFcK2OoH5yw6/4oS+Cd8QjAzEQ38/P3asqEc+lzUtg1qUrddgtf1WOYung3bCzl6gLjGD3DWv82/GiEi7NhUwZnVJhqOh0rBGsjAL+ftpgR+7uPXvXqSMHKBX504VEy5zDs8Xw39nxV5DFrPCrU3v4oo4wc4gxNfAhG0cg/8BmF+tYnnTq/imt2bAHDGWClaMoDNrwabj9uvlg1rrMo8qzh5mq6HTeNFbNnAWX4WZ8+qEnzSGL+Q61R9Hwg6dC5Ubaw0HBmgVxutA+z/efQYfvhj3850/labbqgiGwjyQscX66GNWU0871TaVVSK3Oo7aN055bzdVKnHL+DSGn8Moxv4RXI3QeoRw8XF/1mRx6D1rBCycBrjNyOBX+DC8/gXttiDwP/YUc4yX/fiGQDA0YU6xnw9GABOKFcdaf3481wPkDwcphUYY3jer64FELJwAnwKWSeMPzHwpzD+7RvDvX/UDXOl4UgZSAxvT8Oh06tgDDh8tn3gP7Fcx/kbwusR58vxpTo2Voo43//81MB/4aagiyu3+hoD1bLh8HwVC74VNuqyUzX+ot8gTyOM0Qv8UaknMfDzL3RUHmmHXo5djEI6E2KMP+zqAQIGZxokg0vBzDfgmkR44QwPNC/bPg2AyxzC/7+xUgj1t+mX1NNpH5ZvHziNN/zRPXjIz1XM+8GjXAgcUNOVAkqWkVq9a7seTvmkQfTin1BdPSkav+rhByAD8rHFOlYbLjb7zcbaMX7xPkfzOkk4sViXzytQKfB1uh7DdKUg2b26vijjLw2Q1HNiqY43/tE9+MQ9z6JcMGJXt+HunDq5m4TRC/xKpWJ64BeMP/ji1G23rYc6GMLSj+6crXv1JDH+ybIli7UKOc8LMAzCkn9pfdmWDbG/vXlDGScUhtz7Aq7ukrtP+zbNrz1+HEAg9bzUH36ysVIEEWHrVFky/uOLdTxxdAmnV/h58+n7DuFNf3QP6raLJT+hPVGysHlDGQbFA7zYrLdHpJ6SZWJmooSjCzXO+P0AvZLA+D2PSdeUCPjHF1sTF89jOLnciHWvVLuwTo8FgX97yHFUkO08RB+nQSngevbUChyP9z1KSr4L55u0cw5Ycrduu5mb5PUKIxv4ZcuGhN3+pP/FUaWe3/mnx/Gzf/G9ls8txy72gfETcSdNvFdPUJwioLZaEJ79vDV+8XdnJoqY8hmx+re3TJVDjF8EZKPXyd0OA7/wq9/9JO8lKKSeq3bwq5hN4zzYbZkq4/hiHYs1G6/7w6/j7X/6Lbz1j78JxhgeP7qE5YaDIws1HD5bRaVoYrpSwA9dvhlf+43X44JI4BeSzsXnT8TWc8F0WdpIhdSzEmH8z51exU/82b14ze/9CxardmbGf3q1AcdjIScRwK9A1LkAezZPomQZsQ1LbAhjRQtjBQP1ASngElXGn//3P4DP/OKrY/cHjN+3cw5Yd87f/NwjuPmvH1jXNYxc4BcNmUTlatP1YgO5JeNXvjgPH17E40eWWgaSXg5aT4JJFEvuGgbBoDDjF77syVJBBv68NX4RwAXDEmX8gvFv2VAOBSLxlve8LXOHgf+IHzSePL6MIws1nK02YRokax3E3IKtU2M4tljHs6dW0HQ8XL1zGmdWmzix1MALflL1hTNVHJ6vYuemCoh4MH1RQnB/6bYpfOlXXou9F26M3XfB1Bie8QO/kGRUVw9jDDfdeh8eOryAatPFgVMrkum3Sz6f8I+LSj1EQVfXqbEi3vMDu/ClX3lt6EoACAL/uGT8vQv8jQ6e+8jZGgwCXrx5MrapAcG5IRo1Dhrjf2RuAY/MLcTiUj8xcoG/6fITSLRsYCzcz4UxJrV9sQEwxvDc6VU0XS/kbY6il4PWk2AYFCvgAjjbV2+vKIz/PL9qNOlxa4FgUSLxKVissLZu3lDG6ZWmvOLqtavH6LJyd+5sDbv8BPjdT57E/KqNjZWCDNibJ/nr2zLFN7Jn/aD8Iy/fBgA4eHpF5jpemK/i0JlqSA9PAhHhim1TiYnurdNlWYE6M1GEQeFGbceX6ji+VMe7X7MLAPDMiWWc8d1T7WYGiPujUg8QyD3TlQLGiib2bJ6MHbPzvIo8tmz1TuO/56mTuPJ3vpYpZwHwz3DzhnLqhDRZAzOAbZmbjofD81Us1Z2QsSDa8rzXGL3A7wjGHzQpU3X+lYaDmu1iZqKIapPr+ieWGtKqdlBxfESxVOufj5//HTMxn2AaFGH8IvAXJLsbz3mN4u8JGUMw/rIi9QBB3qTnBVxml4x/oYbX7pnBzk0VfOOpkzi72sTGShFXbJvC/37fq2T3zK1TZTgew77nz8I0CK/bw4cDPXV8WQbUQ2eqeGG+feBvhQsUjVokiNVck2gd8aZLz4dlEL733DwY459HO8YvA38CKxbn1XSkPYeK3f585qmxQk9dPfcePIO67cmEezscWajG8iUqKkUTlkF+AddgtWV+Yb4qHW9idsOJpTpe9jtfwz8/caJv6xi9wO+Gk7sAQju+sBxe4SfzTi7VcfB00JfluVPpPVr2H1vCrvMqPR+0LnDbe16Jn792d+x2y6CYjx/gk7Yu37oBt71nL35wz0yuazGNMOPfOB4k/oCAVQrW1nNXj2D8HVwuL9dtLNZsbN9YwTW7N+HBFxYwX21KeefaF83I9YrXc+/BM9ixcQw7N1VQsgx88+lT8vkeODSPhuNJC2032DodBOXEwO9fcVy2dRI7NlVw77NnAACXbpnEqZVGy43vhO/TjxaUAYE8KK7cknDDVRfgL3/+ldi+sYJysXc+/ieO8uEv0SEwaZg7W4vlI1T85N6d+NtffDWKFmf8g2TnVK3EIvD/8/4TWGk4+MeHY4MNe4bRC/yygMuUCU6V8QtGeoWv6Z5cbsgPwKDgw0jCw3MLuNJPAvYDV+/cmPilNc0I41ekHiLCmy7dHEsKrxUy8PtfuKmxsMYvrjSE/uz1WOPvJrkrOlBumx7DlTu4Zr//2JKsPFYhchkvzFexe2YchkHYPTOOew/ywDszUcQjfvVs1tm8SVBdKeMlCxNlK6TxHzi5jE3jRZw3UcLumXHJ4q/cMQ3XY9JplITjS3XMTpQSN98x37461WK2RMky8cZL+BVQ2TLRdLxMksTHv34AP/HJ72bWsPcfWwYQbACt4Lgeji/WYzURKqYqBVlkWBiwAq6kWHP3fm40+MZTJ/smS41c4JctG/zRi0C41a3wX1+xjVsSTyzV8dypVZQLBi7buiFV6jmxVMexxTqu3N6/wJ8Gzvjj/dWjXTXzhJR6BOOvBEPagUBOOHByBW/+b/fga09wu2Sv7ZzRxJ3rMbzrE9/B39z7fOwxIrG7feMYrvI/x+W6kziIXpVHdvmSx67zxqXc8doXzcgE9pqkHoXxT5Z5EZjat+eZEysy/yCkFwBy/a3knhNLdWxOkHmAQK5sJfWoEJJeuxnWjDF8dt9h3P/8WTzb4upZ4ORyHadXGiAC9h8PAv9/+vuH8f9+8bHY8SeWuVMpWgWdhl5M4VsLDp5exabxInbNjOO506uo2y6+8+xp7NxUwVLdwQMdtpHpFpkCPxFdT0RPEdEBIvpgwv0lIvo7//7vEdEu5b4P+bc/RURvzW/pyViuc69z0TRwte+i+Jf9cgywTOwKqefUcgPPn1nFrvPGcfHshBydF4VoTtZPxp+GqMYvvsTR0vVc/yaFGX/U1bOxUkDRMvDp7x3Cs6dW8diRJRjUh8rdCKv8zoHTePCFBfz5t5+LMc45GfgruGTLpNK2Oh78zhsvysKri/yAu3uW/18pmnj5Tn5uEcUrcjvB+ZNl+VqE1COSvYwxPHNyBXsigb9oGbjE76DaKsF7fLGOLRuS5y2ryd0sEAVu7RK8B0+v4pCf/Fa/d2kQbP+1L5rB4fkalus2ji/W8fnvz+H2+18ItdQAgDnfUdVK41dhGdzgkeektrXgudMr2D0zjov8wH/vszy/8cG3XYqCSfj6k+3fszzQNvATkQng4wDeBuByADcR0eWRw94L4Cxj7EUA/hjA7/uPvRx8OPtLAFwP4BP+8/UErsdw+/2HcemWSWwaL+Li2QlctWMa//D9OXnMiaU6ygXuWS5aBk4uN3Dw9Cp2z4xj98w45s7WEq1lD88twDIIL7lgQ+y+fsMyjESNf7LcO8YvbKSb/arUacn4+WZDRNiyoSyvqIDeyTxAUMcQ/UJ/3v+sD52pxprwHVmooWQZmJkoomgZuML/LDcmyB2GQZL1754JB96dmyrS8XLB1Niacj6mQfI9FT39havn1EoDizVbBn6xAW3eUJK5llZOmONL9URHD6Amd7ONERVyYjtLp5Attmwoy1qJVhC6/rt819STx5fxxYeOwGPcmv2lR8K6tyrXZYEwAQwK639OiTXPn1nFFx86gkrRxJsuPR+v2n0e/mVQAj+AawAcYIwdZIw1AdwO4IbIMTcA+Cv/588BeDNxqncDgNsZYw3G2HMADvjP1xN86ZGjOHhqFb/25j2Saf7Y1dvw5PFlPO73mTm53MD5k2UQETZv4FWTL5zhOu5Fs+NgDNKup+Lhw4u4dOukvORdT1gRjb+cMis3T5gGZ6cidyCCpdoATASZn3rVTgC9K97iz83/VwP/ct3GVx4/jne9fBsqRRP/8MBc6DFzZ6vYtnFMnhtXyqKt5OC31W/WJpi+CLw7NlWkvLNjU7YA1Apbp8dQMLk0qSZ3D/iOHmG1FOvYsqGM8yZKsFo4e6pNB8t1J1XqEZ/bVGbGz49vN4Xr7idP4pLNk/ixV2zDvkNnsVhr3Xdo/7ElXDBVxg9czM0ITxxdwj88MIerd07j0i2T+Nz3j4SOF1dt0SK5NIh6lkEYxrLqOwh54J9A3fbwjw8dxc+++kKUCyZ+Yu92XHf55r4ko7NEim0ADiu/zwF4VdoxjDGHiBYBnOfffl/ksdu6Xm0LeB7D/7j7AF68eQJvfckWefu/edkFuOVLT+A9f3k/pscKOLJQw+VbOdM7f7KMu544Acdj2DUzjl3n8S/We/7y/piN8vkzq/jJV+7oxdI7Rlzq6T3jNw0j5ECJunoAYPNUGUTAr7zpRXj25Aoenstmz+sGgvH/yb88g7/67vMA+ACTuu3hZ159IYiAz3//SEgznTtbw95dQRGVqNZNYvwA1/lLloGt/oYmtP4LN1WwfeMYiMLNzLrFBdNjePbUCogI4yU+W+GHPvoNLPmy5R6lxqBcMLB5A5eHzp8s4dP3HcJdCTZAEejSGP9Y0YRB8elVaRBSz3v+8v5Ym2cVz55awftffzHedOlmfPzrz+Idf/qtWHdQFYfPVnHtxTPYvKGEjZUCPnrX01is2fgv77oC1YaL/3Lnfrzlo9+AONtPrTQwO1nKTMAE43/nx77ds5qSrBBXHbtnxuUV85U7pvEfr7sEAHDDVT0JjYnI8qknvVvR7TPtmCyP5U9AdDOAmwFg586dGZYVRtV28YqdG/H6S2ZDTHPjeBG//Y7L8b3nuBtjz+YJ/PDLLgAAvPe1u/GlR46i7LsXpisF/Myrd8Y6NgLAJVsmcdM1na+rF/jVN+2RHRUB4JW7NuHm112EV+6KV4bmhfe/7qJQNfBLt03j/a+7CNdeHNhGf/bVF+LqndPYOjWGD739Mux7fj7pqXLBxkoB73vtbhyNNFJ7x0u34uqd05gas2C7LNS2ec/mCfz4K7bL39982Wa877W7pQMkip97zYV45e5N8nw6b7yI33zrJXjLZZtRskz89jsuT6zG7RTvfs2FePVFfA0/8vJtOLXSkPmJnZvGZfdTwyD89jsux4v9K4BfftOL8J0Dp1Of9+U7pvGDfv1BFD929Xbs2FjJfFX2yl2b8KNXb2ur8V9+wQb81DU7ccH0GH7h2t04vpReEAnwz+SnX3UhiAj/6a2X4DsHTmOiZOGGq7bBdRmePL6Mmu2Ejn/Nxdmtym+85Hw8+MJCx838eoVXXLgJ1148g1LBwC9cuxu/8NpdfbOHq6B2lisieg2A/8wYe6v/+4cAgDH2X5Vjvuofcy8RWQCOA5gF8EH1WPW4Vn9z7969bN++fV2/KA0NDY1zDUT0AGNsb5Zjs2w19wPYQ0S7iagInqy9I3LMHQDe7f/84wDuZnxHuQPAjb7rZzeAPQD+NcvCNDQ0NDR6g7ZSj6/ZfwDAVwGYAG5jjD1ORLcA2McYuwPAXwD4GyI6AGAefHOAf9xnATwBwAHwy4yxwWjxp6GhoXGOoq3Usx7QUo+GhoZGZ8hb6tHQ0NDQGCHowK+hoaFxjkEHfg0NDY1zDDrwa2hoaJxj0IFfQ0ND4xzDQLp6iOgUgEMdPmwGQHoZ4/piUNem19UZ9Lo6x6CubRTXdSFjLLlUO4KBDPzdgIj2ZbUy9RuDuja9rs6g19U5BnVt/397ZxMaVxXF8d+fVgtqtdQvAq0mERVc2eCioO1GURO08QOkIhioG0HBIoKVgHRbRReCWBSLVaoW0WI2QkVEV63YmDQpaZtWs6gdE+jCCoqIHhf3DryM8/IBmXvHeecHj3fn5A3z53/PO/PenZmcquvypR7HcZyK4YXfcRynYnRS4X87t4AFaFdtrmt5uK7l067aKq2rY9b4HcdxnKXRSVf8juM4zhLoiMK/WDP4hDo2Svpa0pSkE5Kei/Hdkn6WNBa3gQzaZiRNxNf/PsbWS/pS0nTct66TS3NNtxY8GZN0UdLOXH5J2idpTtJkIdbUIwXeiDl3XFJfYl2vSjoZX/uQpHUx3i3pj4J3exPrKp07SS9Fv05Jui+xroMFTTOSxmI8pV9l9SF9jpnZ/3oj/Kvos0AvcCkwDtyWSUsX0BfHa4HThAb1u4EXMvs0A1zTEHsF2BXHu4A9mefxF+DGXH4BW4E+YHIxj4AB4AtCl7nNwNHEuu4FVsfxnoKu7uJxGfxqOnfxPBgH1gA98ZxdlUpXw99fA17O4FdZfUieY51wxb+UZvBJMLOamY3G8W/AFC3qMbxCDAL743g/8FBGLXcDZ81suT/cWzHM7FtCP4kiZR4NAu9b4AiwTlJXKl1mdtjM6j0JjwAb/vPEFlPiVxmDwMdm9qeZ/QScIZy7SXVJEvAY8FErXnshFqgPyXOsEwp/s2bw2YutpG5gE3A0hp6Nt2v7Ui+pRAw4LOmYQn9jgOvNrAYhKYHrMuiqs535J2Nuv+qUedROebeDcGVYp0fSD5K+kbQlg55mc9cufm0BZs1suhBL7ldDfUieY51Q+Jfc0D0Vkq4APgV2mtlF4C3gJuB2oEa41UzNnWbWB/QDz0jamkFDUxRaem4DPomhdvBrMdoi7yQNE7rbHYihGnCDmW0Cngc+lHRlQkllc9cWfgGPM/8CI7lfTepD6aFNYiviWScU/nPAxsLjDcD5TFqQdAlhUg+Y2WcAZjZrZn+b2T/AO7ToFnchzOx83M8Bh6KG2fqtY9zPpdYV6QdGzWw2aszuV4Eyj7LnnaQh4AHgCYuLwnEp5UIcHyOspd+SStMCc9cOfq0GHgEO1mOp/WpWH8iQY51Q+JfSDD4Jcf3wXWDKzF4vxIvrcg8Dk43PbbGuyyWtrY8JHwxOEnwaiocNAZ+n1FVg3lVYbr8aKPNoBHgyfvNiM/Br/XY9BZLuB14EtpnZ74X4tZJWxXEvcDPwY0JdZXM3AmyXtEZST9T1XSpdkXuAk2Z2rh5I6VdZfSBHjqX4NLvVG+HT79OEd+vhjDruItyKHQfG4jYAfABMxPgI0JVYVy/hGxXjwIm6R8DVwFfAdNyvz+DZZcAF4KpCLItfhDefGvAX4WrrqTKPCLfhb8acmwDuSKzrDGH9t55ne+Oxj8Y5HgdGgQcT6yqdO2A4+nUK6E+pK8bfA55uODalX2X1IXmO+S93HcdxKkYnLPU4juM4y8ALv+M4TsXwwu84jlMxvPA7juNUDC/8juM4FcMLv+M4TsXwwu84jlMxvPA7juNUjH8BK8r8Lm3C3tUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21ad7f0400>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of rewards\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,len(scores_history) +1 ), scores_history)\n",
    "\n",
    "#plt.plot(range(1,len(scores) +1 ), scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agent' from '/home/workspace/agent.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
